{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellPLATO | Cell Plasticity Analysis Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellPLATO as cp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "OVERWRITE_DATAFRAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment list from the experiments listed in the config \n",
    "exp_list = cp.populate_experiment_list()\n",
    "display(exp_list)\n",
    "print(cp.SAVED_DATA_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure shape and motion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, process and combine the dataframes (including segmentation and migration calculations)\n",
    "comb_df = cp.combine_dataframes(exp_list)\n",
    "\n",
    "comb_df, new_factors = cp.measurement_pipeline(comb_df, mixed=cp.MIXED_SCALING, factors_to_timeaverage = cp.ALL_FACTORS) #If AVERAGE_TIME_WINDOWS is true, then the comb df contains new factors, which are output as 'new factors'\n",
    "display(new_factors)\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "comb_df, filt_counts = cp.apply_filters(comb_df)\n",
    "\n",
    "# Process a time-averaged DataFrame\n",
    "tavg_df = cp.time_average(comb_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save newly created dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    comb_df.to_csv(cp.SAVED_DATA_PATH + 'comb_df.csv', index=False)\n",
    "    tavg_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option to load dataframes if this step is already complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.read_csv(cp.SAVED_DATA_PATH + 'comb_df.csv')\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_values = cp.getaveragevalues(df_in=comb_df, factorstoinclude=cp.ALL_FACTORS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp.comparative_visualization_pipeline(comb_df, num_factors=cp.ALL_FACTORS) #usually this is done on dr_df so it has all the labels like tsnes and umaps\n",
    "cp.comparative_visualization_pipeline(comb_df, num_factors=cp.ALL_FACTORS) #usually this is done on dr_df so it has all the labels like tsnes and umaps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction (3D UMAP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform correlation analysis to understand which factors correlate to one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = comb_df\n",
    "# FACTORSTOINCLUDE = only_tmeans\n",
    "cp.correlation_matrix_heatmap(df_in, factors = cp.ALL_FACTORS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance thresholder to decide on input factors for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKDR_FACTORS = cp.variance_threshold(comb_df, threshold_value=0.03)\n",
    "# Optionally, define new dr factors here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REGIONPROPS_LIST = ['area',\n",
    "                    # 'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    # 'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    # 'minor_axis_length',\n",
    "                    'orientation',\n",
    "                    'perimeter',\n",
    "                    #  'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                # 'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                # 'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "new_DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONPROPS_LIST = ['area',\n",
    "                    'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    'orientation',\n",
    "                    'perimeter',\n",
    "                     'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "new_DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP DR (and tSNE and PCA) and then cluster analysis on the comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User can change the umap_nn to achieve good separation of clusters\n",
    "\n",
    "tsne_perp=150\n",
    "umap_nn = 80\n",
    "min_dist = 0.0 \n",
    "n_components = 3\n",
    "\n",
    "dr_df = cp.dr_pipeline_multiUMAPandTSNE(comb_df, \n",
    "                    dr_factors=new_DR_FACTORS,#can use cp.DR_FACTORS to use the default list\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) # log2minmax # powertransformer #minmax #standard #robust #choice \n",
    "\n",
    "# lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=800,min_samples=400,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    "lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=800,min_samples=400,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    "lab_dr_df.name='lab_dr_df'\n",
    "name = lab_dr_df.name\n",
    "\n",
    "cp.plot_3D_scatter(lab_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name + '_byCLUSTERID___',dotsize = 5, alpha=0.1, markerscale = 18) #color = label or condition   \n",
    "cp.plot_3D_scatter(lab_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier=name + '_byCONDITION___',dotsize = 5, alpha=0.1, markerscale = 18) #color = label or condition  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the new dataframes to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    dr_df.to_csv(cp.SAVED_DATA_PATH + 'dr_df.csv', index=False)\n",
    "    lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv', index=False)\n",
    "    exemplar_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'dr_df.csv')\n",
    "lab_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive 3D UMAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.interactive_plot_3D_UMAP(df=lab_dr_df,colorby = 'Condition_shortlabel', symbolby = 'Condition_shortlabel', what = ' AllTimeUMAPwithclusters') # TavgUMAPwithclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_dr_df\n",
    "\n",
    "#get unique list of conditions from df\n",
    "condlist = df['Condition_shortlabel'].unique().tolist()\n",
    "pickedcondition = condlist[0]\n",
    "\n",
    "cp.interactive_umap_plot_choosecondition(df=lab_dr_df, condition = pickedcondition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot out the UMAP subplots coloured by scaled metric and condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = False, samplethedf = False)\n",
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = True, samplethedf = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP DR (and tSNE and PCA) and then cluster analysis on the tavg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perp=150\n",
    "umap_nn = 20#4#60\n",
    "min_dist = 0.0 #0.15 \n",
    "n_components = 3\n",
    "\n",
    "tavg_dr_df = cp.dr_pipeline_multiUMAPandTSNE(tavg_df, \n",
    "                    dr_factors=new_DR_FACTORS,# new_DR_FACTORS # DR_FACTORS #only_tmeans # cp.DR_FACTORS\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) \n",
    "\n",
    "lab_tavg_dr_df, exemplar_tavg_df=cp.hdbscan_clustering(tavg_dr_df, min_cluster_size=50,min_samples=50,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    "lab_tavg_dr_df.name='lab_tavg_dr_df'\n",
    "name2 = lab_tavg_dr_df.name\n",
    "cp.plot_3D_scatter(lab_tavg_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name2 + '_byCLUSTERID___',dotsize = 30, alpha=0.5, markerscale = 5) #color = label or condition   \n",
    "cp.plot_3D_scatter(lab_tavg_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier=name2 + '_byCONDITION___',dotsize = 30, alpha=0.5, markerscale = 5) #color = label or condition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_dr_df.csv', index=False)\n",
    "    lab_tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv', index=False)\n",
    "    exemplar_tavg_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_dr_df.csv')\n",
    "lab_tavg_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv')\n",
    "exemplar_tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the tavg time averaged cluster labels to the regular df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this function to put the labels into the lab_tavg_lab_dr_df. Slow function. Can potentially be sped up.\n",
    "\n",
    "lab_tavg_lab_dr_df=cp.add_tavglabel_todf(lab_dr_df, lab_tavg_dr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tavg_lab_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster counting - how many cells per cluster ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_tavg_lab_dr_df\n",
    "label_counts_df = cp.get_label_counts(df)\n",
    "\n",
    "cp.plot_label_counts(label_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeinclus_df=cp.count_time_in_label(tptlabel_dr_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(lab_dr_df) \n",
    "display(cluster_purity_df)\n",
    "f = cp.purityplot_percentcluspercondition(lab_dr_df, cluster_purity_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deprecated version\n",
    "\n",
    "# clust_sum_df = cp.cluster_purity(lab_dr_df)  \n",
    "# display(clust_sum_df)\n",
    "# f = cp.purity_plots_dev(lab_dr_df, clust_sum_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = cp.count_cluster_changes_with_tavg(lab_tavg_lab_dr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    # ultimate DF with all of the things you want in it\n",
    "    \n",
    "    tptlabel_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "    # tptlabel_dr_df_compare.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_compare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tptlabel_dr_df\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv') #spidey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes(df, identifier='\\_allcells', maxy=4) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_countplots(df, identifier='_allcells')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate the clusters of cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### both disambiguate_timepoint() and disambiguate_tavg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have them, load them in here\n",
    "\n",
    "lab_tavg_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv')\n",
    "exemplar_tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv')\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take 5 exemplars from each cluster ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of exemplars to look at\n",
    "n=15\n",
    "exemplar_df = exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=220 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, average_df=cp.contribution_to_clusters_topdictionary(df_in=tptlabel_dr_df,  howmanyfactors=10) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the version based on finding TIME AVERAGED exemplars\n",
    "### clusters are defined based on the time averaged data, and found in the not time averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an exemplars DF that only contains 5 of each thing\n",
    "tavg_exemplar_df=exemplar_tavg_df\n",
    "n=15\n",
    "tavg_exemplar_df = tavg_exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=220\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=tavg_exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df=cp.contribution_to_clusters_topdictionary(df_in=lab_tavg_dr_df,  howmanyfactors=10) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "# plot_cluster_values(top_dictionary, df) For this to work on the wholetrack version, you would have to use the tavg labels not the labels\n",
    "wholetrack_exemplar_df=cp.disambiguate_tavg(df, exemp_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of metric magnitude (center scaled) per cluster ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be updated to have the same new scaling as the other plots\n",
    "cp.clustering_heatmap(df_in=tptlabel_dr_df, dr_factors=DR_FACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which metrics contributed the most to each cluster?\n",
    "# Check that this is now redundant with the new function contribution to clusters top dictionary\n",
    "\n",
    "cp.contribution_to_clusters(df_in=tptlabel_dr_df, threshold_value=0.001) #0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellPLATO_gitversion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8276cfd2c083cef360060a25ccdcdda22a83f7dfd426f4d510c031d0a85beba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
