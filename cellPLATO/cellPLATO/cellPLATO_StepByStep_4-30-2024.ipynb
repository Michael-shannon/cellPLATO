{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellPLATO | Cell Plasticity Analysis Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fill in the config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, make sure your data is set up in the following two tiered format:\n",
    "\n",
    "        Master\n",
    "            ├── Condition 1\n",
    "            │   ├── Replicate 1\n",
    "            |   |       ├── tracks.h5\n",
    "            │   ├── Replicate 2\n",
    "            |   |       ├── tracks.h5            \n",
    "            │   └── Replicate 3\n",
    "            |           └── tracks.h5            \n",
    "            │  \n",
    "            └── Condition 2,\n",
    "                ├── Replicate 1\n",
    "                |       ├── tracks.h5\n",
    "                ├── Replicate 2\n",
    "                |       ├── tracks.h5            \n",
    "                └── Replicate 3\n",
    "                        └── tracks.h5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Set your kernel to 'cellPLATO' before continuing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>1. Start by importing packages for cellPLATO</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes cellPLATO itself, and all of the modules you will need\n",
    "\n",
    "* Import these packages, checking that you have them\n",
    "* We're also importing a lot of the modules in cellPLATO, if this cell runs successfully, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellPLATO as cp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "OVERWRITE_DATAFRAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#get matplotlib version\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your experiment list\n",
    "\n",
    "Check that the list generated in the next cell contains your conditions and replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment list from the experiments listed in the config \n",
    "exp_list = cp.populate_experiment_list()\n",
    "display(exp_list)\n",
    "print(cp.SAVED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>2. Measurements of morphology and migration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell does migration and morphology measurements for all of the cells at each timepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, process and combine the dataframes (including segmentation and migration calculations)\n",
    "comb_df = cp.combine_dataframes(exp_list)\n",
    "\n",
    "comb_df, new_factors = cp.measurement_pipeline(comb_df, mixed=cp.MIXED_SCALING, factors_to_timeaverage = cp.ALL_FACTORS) \n",
    "display(new_factors)\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "comb_df, filt_counts = cp.apply_filters(comb_df)\n",
    "\n",
    "# Process a time-averaged DataFrame\n",
    "tavg_df = cp.time_average(comb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make plots for a chosen factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Factor list: \n",
    "all_factors =  ['area',\n",
    "                'bbox_area',\n",
    "                'eccentricity',\n",
    "                'equivalent_diameter',\n",
    "                'extent',\n",
    "                'filled_area',\n",
    "                'major_axis_length',\n",
    "                'minor_axis_length',\n",
    "                'orientation',\n",
    "                'perimeter',\n",
    "                'solidity',\n",
    "                'euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient'\n",
    "                'aspect',\n",
    "                'rip_p',\n",
    "                'rip_K',\n",
    "                'rip_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_factor='area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a timeplot of the chosen factor (mean, 95% CI) for each condition\n",
    "\n",
    "f=cp.multi_condition_timeplot(comb_df, factor = chosen_factor)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculated the effect size for each condition relative to the control defined in the config\n",
    "\n",
    "cp.timeplots_of_differences(comb_df_filt,factor=chosen_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes a comparative plot of difference based on the time averaged dataframe\n",
    "\n",
    "f=cp.plots_of_differences_sns(tavg_df,factor=chosen_factor)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then gives you the option to save the dataframes into your automatically created 'saved data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    comb_df.to_csv(cp.SAVED_DATA_PATH + 'comb_df.csv', index=False)\n",
    "    tavg_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load in precreated dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.read_csv(cp.SAVED_DATA_PATH + 'comb_df.csv')\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: do filtering on the data (on top of what has been stated in the config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined filters in dict {factor:(min, max)}\n",
    "\n",
    "data_filters = {\n",
    "#   \"speed\": (10, 100),\n",
    "  \"ntpts\": (8, 60),\n",
    "#    \"frame\": (0, 450), # Warning: range will change if self-normalized\n",
    "#   \"ntpts\": (12,1800)\n",
    "}\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "filt_df, filt_counts = cp.apply_filters(comb_df,how='any', filter_dict=data_filters)\n",
    "\n",
    "fig = cp.visualize_filtering(filt_df, filt_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all metrics\n",
    "\n",
    "This cell makes comparative plots for every single metric and saves them in your output folder\n",
    "\n",
    "* Plots of difference\n",
    "* Timeplots of difference\n",
    "* Marginal xy plots\n",
    "* Simple bar plots\n",
    "* Superplots - useful for comparing between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Check that you are happy with your extra filtering before continuing\n",
    "Run the next cell on the filtered dataframe or the unfiltered dataframe once you are ready\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs plots of all metrics for all factors\n",
    "cp.comparative_visualization_pipeline(comb_df, num_factors=all_factors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>3. Definition of single timepoint behavioural clusters using UMAP and HDBSCAN</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: for new datasets perform correlation analysis to understand which factors correlate to one another\n",
    "\n",
    "This may aid in choosing the most important factors, aiding clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = comb_df\n",
    "cp.correlation_matrix_heatmap(df_in, factors = cp.ALL_FACTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: use variance thresholder for further insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dr_factors = cp.variance_threshold(comb_df, threshold_value=0.03)\n",
    "chosen_dr_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: define a new list of dr_factors to use for UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONPROPS_LIST = ['area',\n",
    "                    'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    'orientation',\n",
    "                    'perimeter',\n",
    "                     'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Take a sample of cells from each Replicate_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "uniq_ids = comb_df.groupby('Replicate_ID')['uniq_id'].unique().tolist()\n",
    "# Create a new list to store the sampled unique IDs\n",
    "sampled_uniq_ids = []\n",
    "# Iterate over each replicate ID\n",
    "for i, replicate_id in enumerate(uniq_ids):\n",
    "    total_sample_size_counter.append(total_sample_size)\n",
    "    sample_size=len(replicate_id) // 2\n",
    "    sampled_ids = random.sample(replicate_id.tolist(), sample_size)\n",
    "    sampled_uniq_ids.extend(sampled_ids)\n",
    "\n",
    "print(len(sampled_uniq_ids))\n",
    "# print(sampled_uniq_ids.shape())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "uniq_ids = comb_df.groupby('Replicate_ID')['uniq_id'].unique().tolist()\n",
    "\n",
    "# Create a new list to store the sampled unique IDs\n",
    "sampled_uniq_ids = []\n",
    "\n",
    "for condition in comb_df['Condition'].unique():\n",
    "    condition_df = comb_df[comb_df['Condition'] == condition]\n",
    "    uniq_ids = condition_df.groupby('Replicate_ID')['uniq_id'].unique().tolist()\n",
    "    \n",
    "    for replicate_id in condition_df['Replicate_ID'].unique():\n",
    "\n",
    "        replicate_df = condition_df[condition_df['Replicate_ID'] == replicate_id]\n",
    "        rep_uniq_ids = replicate_df['uniq_id'].unique().tolist()\n",
    "        sample_size = len(rep_uniq_ids) // 2\n",
    "        sampled_ids = random.sample(rep_uniq_ids, sample_size)\n",
    "        sampled_uniq_ids.extend(sampled_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform UMAP and cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, do UMAP, save the new df and plot the UMAP\n",
    "\n",
    "Well separated clusters depend mostly on 1. the input factors and 2. the umap_nn setting\n",
    "\n",
    "You can change both, depending on the nature of your data, in order to achieve a reasonable level of separation of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### User alterable parameters ######\n",
    "tsne_perp=150\n",
    "umap_nn = 50 #umap nearest neighbours\n",
    "min_dist = 0.0 #umap minimum distance (usually keep this at 0 or very low)\n",
    "n_components = 3 # number of umap dimensions to calculate\n",
    "#######################################\n",
    "\n",
    "\n",
    "# for umap_nn in [40,50, 60, 70, 80, 90, 100]:\n",
    "\n",
    "dr_df = cp.dr_pipeline_multiUMAPandTSNE(comb_df, \n",
    "                    dr_factors=DR_FACTORS,\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',\n",
    "                    do_tsne=False) # A number of scaling methods are available: 'choice', 'minmax', 'standard', 'robust', 'normalize', 'quantile', 'maxabs', 'yeo-johnson', 'box-cox'\n",
    "\n",
    "dr_df.to_csv(cp.SAVED_DATA_PATH + 'dr_df_'+ str(umap_nn) + '.csv', index=False) # Saves the df\n",
    "\n",
    "cp.plot_3D_scatter(dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier='dr_df_' + str(umap_nn), dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dr_df\n",
    "dr_df.to_csv(cp.SAVED_DATA_PATH + 'dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User adjustable parameters #####\n",
    "min_cluster_size = 800\n",
    "min_samples = 300 #300 #500\n",
    "cluster_by = 'UMAPNDIM' # UMAPNDIM = default, clusters on UMAPs. NDIM = alternate, clusters on all dimensions\n",
    "metric = 'euclidean' # See https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan.HDBSCAN for options\n",
    "#######################################\n",
    "\n",
    "lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_by=cluster_by,  metric=metric)\n",
    "\n",
    "lab_dr_df.name='lab_dr_df'\n",
    "name = lab_dr_df.name\n",
    "\n",
    "lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df' + str(umap_nn)+str(min_samples)+'.csv', index=False)\n",
    "exemplar_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df' + str(umap_nn)+str(min_samples)+'.csv', index=False)\n",
    "\n",
    "cp.plot_3D_scatter(lab_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name + 'thisone'+str(umap_nn)+str(min_samples),dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then plot the 'fingerprint' plot of percentage in each cluster per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new combo\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(lab_dr_df) \n",
    "display(cluster_purity_df)\n",
    "f = cp.purityplot_percentcluspercondition(lab_dr_df, cluster_purity_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: explore the clusters with interactive 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.interactive_plot_3D_UMAP(df=lab_dr_df,colorby = 'Condition_shortlabel', symbolby = 'Condition_shortlabel', what = ' AllTimeUMAPwithclusters') # TavgUMAPwithclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: all other conditions colored grey, chosen condition in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_dr_df\n",
    "\n",
    "condlist = df['Condition_shortlabel'].unique().tolist() #get unique list of conditions from df\n",
    "print(condlist) # show the condition list\n",
    "# chosen_condition = '' #specify a chosen condition from the list\n",
    "chosen_condition = condlist[0] # or choose the first one\n",
    "print(chosen_condition)\n",
    "\n",
    "cp.interactive_plot_3D_UMAP_chosen_condition(df, chosen_condition, opacity_grey=0.01, marker_size_all=2,) #change opacity and marker size to suit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make UMAP plots colored by metric contributors - the more intense the color, the higher the contribution the metric to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one colors per metric\n",
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = False, samplethedf = False)\n",
    "#second one colors per condition\n",
    "# cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=tptlabel_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "#                                                    identifier='inferno', colormap='inferno', coloredbycondition = True, samplethedf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP then HDBSCAN on the tavg_df (deprecated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step used to add tavg_label to the dataframe. Its deprecated and can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perp=150\n",
    "umap_nn = 20#4#60\n",
    "min_dist = 0.0 #0.15 \n",
    "n_components = 3\n",
    "\n",
    "tavg_dr_df = cp.dr_pipeline_multiUMAPandTSNE(tavg_df, \n",
    "                    dr_factors=newnew_DR_FACTORS,# new_DR_FACTORS # DR_FACTORS #only_tmeans # cp.DR_FACTORS\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) # log2minmax # powertransformer #minmax\n",
    "\n",
    "lab_tavg_dr_df, exemplar_tavg_df=cp.hdbscan_clustering(tavg_dr_df, min_cluster_size=50,min_samples=50,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    "\n",
    "#Run this function to put the labels into the lab_tavg_lab_dr_df. Slow function. Can update search by uniq_id alone...\n",
    "\n",
    "lab_tavg_lab_dr_df=cp.add_tavglabel_todf(lab_dr_df_ACTUAL, lab_tavg_dr_df)\n",
    "lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, just make lab_tavg_dr_df by adding random labels to a new column called tavg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_dr_df['tavg_label'] = np.random.randint(0, 5, lab_dr_df.shape[0])\n",
    "lab_tavg_lab_dr_df = lab_dr_df\n",
    "# lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Save your dataframes so you can come back to this step if necessary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_dr_df.csv', index=False)\n",
    "    lab_tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv', index=False)\n",
    "    exemplar_tavg_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the plasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell counts the number of single timepoint cluster switches, creating new dataframe tptlabel_dr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = cp.count_cluster_changes_with_tavg(lab_tavg_lab_dr_df)\n",
    "tptlabel_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_changes(df, identifier='\\_allcells', maxy=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_countplots(df, identifier='_allcells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_allcells', miny=None, maxy=None, t_window_multiplier = cp.T_WINDOW_MULTIPLIER, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-abstractify the clusters!\n",
    "This set of functions makes plots to describe the nature of cells from each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, choose a number of exemplar cells to pick out from the exemplar cell list to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of exemplars to display for each cluster\n",
    "n=5\n",
    "exemplar_df = exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show exemplar cells overlaid onto raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - you must have the raw data in the folder with the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.make_raw_cell_png_overlaidwith_behaviourcontourandtrack(tptlabel_dr_df, exemplar_df, LUTlow=5, LUThi=140, XYRange = 200, invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make a full de-abstractification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions \n",
    "1. Rank features based on their contribution to the formation of each cluster\n",
    "2. compute averages of each feature per cluster and print a table\n",
    "3. Makes per cell plots over time of a) top contributing features, b) cluster switches and c) contour maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=220 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=3, dr_factors= DR_factors) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plots plasticity changes in a subset of exemplar cells over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User inputs ####\n",
    "whole_df = tptlabel_dr_df\n",
    "exemplar_df = exemplar_df\n",
    "numberofdesiredtimepoints = int(whole_df['ntpts'].mean())\n",
    "numberofcellspercluster = 40\n",
    "num_clusters_whole_dataset = len(whole_df['label'].unique())\n",
    "# override = int((numberofcellspercluster*num_clusters_whole_dataset)*0.7)\n",
    "#####################\n",
    "\n",
    "exemplar_df_filt, exemplar_cell_tracks_df = cp.filter_exemplars(whole_df=whole_df, exemplar_df = exemplar_df, numberofdesiredtimepoints = numberofdesiredtimepoints, \n",
    "                                                                    numberofcellspercluster = numberofcellspercluster, override = override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=exemplar_cell_tracks_df\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot any factor as small multiples from the exemplars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives a line plot of any feature desired from the exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exemplar_cell_tracks_df\n",
    "whichcolumntoplot = 'label'\n",
    "\n",
    "cp.plot_small_multiples(df, whichcolumntoplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>4. Trajectory measurement: Damerau-Levenshtein</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part measures the similarity of sequences of single timepoint cluster IDs, in order to create trajectories of behaviour that can be compared between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter the tptlabel_dr_df to include only a subset of data of similar timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 180\n",
    "high = 220\n",
    "\n",
    "tptlabel_dr_df_filt = tptlabel_dr_df[tptlabel_dr_df['ntpts'].between(low, high)]\n",
    "print(f'You are sampling {len(tptlabel_dr_df_filt.uniq_id.unique())} cells')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the filtered data reflects the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorchoice = 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes timeplots of the unfiltered and filtered data\n",
    "\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df, factorchoice)\n",
    "f.show()\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df_filt, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of difference of the unfiltered and filtered data\n",
    "f = cp.plots_of_differences_sns(tavg_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_trajectory_df = cp.time_average(tptlabel_dr_df_filt)\n",
    "f = cp.plots_of_differences_sns(tavg_trajectory_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Damerau-Levenshtein analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt\n",
    "distance_matrix_dameraulev = cp.calculate_edit_distances(df,distancemetric = 'dameraulev', print_interval=10000) #fastdtw # dameraulev # mongeelkan\n",
    "print(distance_matrix_dameraulev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a UMAP/HDBSCAN parameter sweep, and select plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sweep through a number of UMAP/HDBSCAN settings to ensure good clustering '''\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "for n_neighbors in [8, 10, 12]:\n",
    "    for min_samples in [5,8,10, 15, 30, 40]:\n",
    "        for min_cluster_size in [5,8,10, 15, 30, 40]:\n",
    "            print(f'min_samples = {min_samples}')\n",
    "            print(f'min_cluster_size = {min_cluster_size}')\n",
    "            print(f'n_neighbors = {n_neighbors}')\n",
    "            tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    "             do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Chosen UMAP and HDBSCAN parameters'''\n",
    "\n",
    "min_samples = 10\n",
    "min_cluster_size = 5\n",
    "n_neighbors = 8\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "\n",
    "print(f'min_samples = {min_samples}')\n",
    "print(f'min_cluster_size = {min_cluster_size}')\n",
    "print(f'n_neighbors = {n_neighbors}')\n",
    "tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    " do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the -1 TRAJECTORY cluster if required\n",
    "# tptlabel_dr_df_filt_clusteredtrajectories = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['trajectory_id'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fingerprint plot of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(df, cluster_label='trajectory_id') \n",
    "f = cp.purityplot_percentcluspercondition(df, cluster_purity_df, cluster_label='trajectory_id', dotsize = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disambiguate the trajectory clustered cells:\n",
    " 1) Make an exemplar_df_trajectories containing example rows\n",
    " 2) Get the full tracks from those rows and make exemplar_df_trajectories_fulltrack\n",
    " 2) Disambiguate with exemplar_df_trajectories\n",
    " 3) Plot multiples with exemplar_df_trajectories_fulltrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "exemplar_df_trajectories, exemplar_df_trajectories_fulltrack  = cp.make_exemplar_df_basedon_trajectories(df, cells_per_traj=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_tracks_df = pd.read_csv(cp.SAVED_DATA_PATH + 'full_tracks_df.csv')\n",
    "df = exemplar_df_trajectories_fulltrack\n",
    "cp.plot_trajectories(df=exemplar_df_trajectories_fulltrack, global_y=True, global_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: full deabstractification of trajectory clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "\n",
    "df= tptlabel_dr_df_filt_clusteredtrajectories \n",
    "exemp_df=exemplar_df_trajectories \n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=2, dr_factors= DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent fingerprint plot for cluster IDs per TRAJECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.fingerprintplot_clusters_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasticity of cells per trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.plasticity_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plasticity_per_condition(df):\n",
    "\n",
    "    # Set the style and color palette\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    max_values = df.groupby(['Condition_shortlabel', 'uniq_id'])['cum_n_changes'].max().reset_index()\n",
    "    display(max_values)\n",
    "    median_values = max_values.groupby(['Condition_shortlabel'])['cum_n_changes'].median().reset_index()\n",
    "\n",
    "    colors = []\n",
    "    cmap = cm.get_cmap(cp.CONDITION_CMAP)\n",
    "    numcolors=len(df['Condition_shortlabel'].unique())\n",
    "    for i in range(numcolors):\n",
    "        colors.append(cmap(i))    \n",
    "\n",
    "    plt.figure(figsize=(15,10))  # Adjust the size of the plot\n",
    "\n",
    "    barplot = sns.barplot(x='Condition_shortlabel', y='cum_n_changes', data=median_values, color='grey', alpha = 0.5)\n",
    "    sns.violinplot(x='Condition_shortlabel', y='cum_n_changes', data=max_values, palette=colors)\n",
    "\n",
    "    # Add the median values to the plot\n",
    "    for i, bar in enumerate(barplot.patches):\n",
    "        barplot.annotate(format(median_values['cum_n_changes'].values[i], '.2f'), \n",
    "                        (bar.get_x() + bar.get_width() / 2, bar.get_height()), \n",
    "                        ha = 'center', va = 'center', \n",
    "                        xytext = (0, -10),  \n",
    "                        textcoords = 'offset points',\n",
    "                        fontsize = 35)\n",
    "    # Increase the font size\n",
    "    plt.xticks(fontsize=35)\n",
    "    plt.yticks(fontsize=35)\n",
    "    # Increase font size of x and y labels\n",
    "    plt.xlabel('', fontsize=35)\n",
    "    plt.ylabel('Cumulative cluster switches', fontsize=35)\n",
    "    # set the ylim\n",
    "    plt.ylim(-10, 150)\n",
    "    # save it in the trajectory disambig folder\n",
    "    # plt.savefig(TRAJECTORY_DISAMBIG_DIR + 'plasticity_per_trajectory.png', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasticity_per_condition(tptlabel_dr_df_filt_clusteredtrajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes_trajectories(df, identifier='\\_allcells', maxy=9 , t_window_multiplier = 1) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all newly created dataframes\n",
    "tptlabel_dr_df_filt_clusteredtrajectories.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories.csv', index=False)\n",
    "exemplar_df_trajectories.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df_trajectories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in exemplar_df_trajectories\n",
    "exemplar_df_trajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df_trajectories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animations of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in tptlabel_dr_df_filt_clusteredtrajectories\n",
    "tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_trajectory_animations(df, exemplar_df_trajectories, number_of_trajectories=10, colormode='trajectory') # singlecluster, cluster, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a number of example cells from each trajectory ID to map back on to the data and display as stacks of PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trajectories = 30 # Select a number of trajectories to plot\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "trajectory_ids = df['trajectory_id'].unique()\n",
    "\n",
    "uniq_id_choices_list = []\n",
    "\n",
    "for trajectory_id_choice in trajectory_ids:\n",
    "    # for each trajectory_id, get a list of possible uniq_ids from the df\n",
    "    uniq_id_choices = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['trajectory_id']==trajectory_id_choice]['uniq_id'].values\n",
    "    # Make sure each once is unique in that list\n",
    "    uniq_id_choices = np.unique(uniq_id_choices)\n",
    "    # choose a number of random uniq_ids from that list based on number_of_trajectories\n",
    "    uniq_id_choices = np.random.choice(uniq_id_choices, number_of_trajectories)\n",
    "    # append each choice to a list\n",
    "    uniq_id_choices_list.append(uniq_id_choices)\n",
    "# flatten the list\n",
    "chosen_uniq_ids = [item for sublist in uniq_id_choices_list for item in sublist]\n",
    "    \n",
    "print(f'There are {len(chosen_uniq_ids)} cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the df to only include the chosen uniq_ids\n",
    "subset_trajectories_df = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['uniq_id'].isin(chosen_uniq_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function makes png image sequences of cells belonging to each trajectory and saves them for visualization. By changing the colormode from trajectory to cluster, you can color the cells by single timepoint cluster or trajectory cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset_trajectories_df\n",
    "cp.make_png_behaviour_trajectories(df,chosen_uniq_ids,XYRange = 300, follow_cell = False, invert=False, colormode = 'trajectory', snapshot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset_trajectories_df\n",
    "cp.make_png_behaviour_trajectories(df,chosen_uniq_ids,XYRange = 300, follow_cell = False, invert=False, colormode = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes a png stack of the raw images to match the above contour maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset_trajectories_df\n",
    "cp.make_raw_cell_pngstacks(df,chosen_uniq_ids,XYRange = 300, follow_cell=False, invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STITCHING FUNCTION - its hard to manually visualize all of the trajectories for each cell, so this function allows one to make a big stitched image for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def stitch_and_save_image_sequence_varying_sizes(master_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    subfolders = [os.path.join(master_folder, f) for f in os.listdir(master_folder) if os.path.isdir(os.path.join(master_folder, f))]\n",
    "    subfolders.sort()  # Ensure consistent order\n",
    "    \n",
    "    # Load all images to determine individual sizes\n",
    "    all_images = {}\n",
    "    for subfolder in subfolders:\n",
    "        images = [f for f in os.listdir(subfolder) if f.endswith('.png')]\n",
    "        images.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # Sorting by time point\n",
    "        all_images[subfolder] = [Image.open(os.path.join(subfolder, img)) for img in images]\n",
    "    \n",
    "    # Determine the number of images (time points) based on the first subfolder (assuming consistent across subfolders)\n",
    "    num_images = len(all_images[subfolders[0]])\n",
    "\n",
    "    for i in range(num_images):\n",
    "        row_heights = []\n",
    "        max_width = 0\n",
    "        images_to_stitch = []\n",
    "\n",
    "        for row in range(0, len(subfolders), 4):\n",
    "            row_images = []\n",
    "            row_max_height = 0\n",
    "            row_width = 0\n",
    "\n",
    "            for col in range(4):\n",
    "                idx = row + col\n",
    "                if idx < len(subfolders):\n",
    "                    images = all_images[subfolders[idx]]\n",
    "                    if i < len(images):\n",
    "                        img = images[i]\n",
    "                        row_images.append(img)\n",
    "                        row_max_height = max(row_max_height, img.height)\n",
    "                        row_width += img.width\n",
    "            \n",
    "            max_width = max(max_width, row_width)\n",
    "            row_heights.append(row_max_height)\n",
    "            images_to_stitch.append(row_images)\n",
    "\n",
    "        # Calculate total grid height\n",
    "        total_height = sum(row_heights)\n",
    "        final_image = Image.new('RGB', (max_width, total_height))\n",
    "\n",
    "        y_offset = 0\n",
    "        for row_idx, row_images in enumerate(images_to_stitch):\n",
    "            x_offset = 0\n",
    "            for img in row_images:\n",
    "                final_image.paste(img, (x_offset, y_offset))\n",
    "                x_offset += img.width\n",
    "            y_offset += row_heights[row_idx]\n",
    "\n",
    "        output_filename = os.path.join(output_folder, f\"stitched_{i+1:04d}.png\")\n",
    "        final_image.save(output_filename)\n",
    "\n",
    "# Example usage\n",
    "# stitch_and_save_image_sequence_varying_sizes('path/to/master_folder', 'path/to/output_folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def stitch_and_save_image_sequence_varying_sizes(master_folder, output_folder, reduce_size=False):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    subfolders = [os.path.join(master_folder, f) for f in os.listdir(master_folder) if os.path.isdir(os.path.join(master_folder, f))]\n",
    "    subfolders.sort()  # Ensure consistent order\n",
    "    \n",
    "    # Load all images to determine individual sizes\n",
    "    all_images = {}\n",
    "    for subfolder in subfolders:\n",
    "        images = [f for f in os.listdir(subfolder) if f.endswith('.png')]\n",
    "        images.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # Sorting by time point\n",
    "        all_images[subfolder] = [Image.open(os.path.join(subfolder, img)) for img in images]\n",
    "    \n",
    "    # Determine the number of images (time points) based on the first subfolder\n",
    "    num_images = len(all_images[subfolders[0]])\n",
    "\n",
    "    for i in range(num_images):\n",
    "        row_heights = []\n",
    "        max_width = 0\n",
    "        images_to_stitch = []\n",
    "\n",
    "        for row in range(0, len(subfolders), 4):\n",
    "            row_images = []\n",
    "            row_max_height = 0\n",
    "            row_width = 0\n",
    "\n",
    "            for col in range(4):\n",
    "                idx = row + col\n",
    "                if idx < len(subfolders):\n",
    "                    images = all_images[subfolders[idx]]\n",
    "                    if i < len(images):\n",
    "                        img = images[i]\n",
    "                        row_images.append(img)\n",
    "                        row_max_height = max(row_max_height, img.height)\n",
    "                        row_width += img.width\n",
    "            \n",
    "            max_width = max(max_width, row_width)\n",
    "            row_heights.append(row_max_height)\n",
    "            images_to_stitch.append(row_images)\n",
    "\n",
    "        # Calculate total grid height\n",
    "        total_height = sum(row_heights)\n",
    "        final_image = Image.new('RGB', (max_width, total_height))\n",
    "\n",
    "        y_offset = 0\n",
    "        for row_idx, row_images in enumerate(images_to_stitch):\n",
    "            x_offset = 0\n",
    "            for img in row_images:\n",
    "                final_image.paste(img, (x_offset, y_offset))\n",
    "                x_offset += img.width\n",
    "            y_offset += row_heights[row_idx]\n",
    "\n",
    "        if reduce_size:\n",
    "            # Reduce the size to a quarter (half the width and half the height)\n",
    "            final_image = final_image.resize((final_image.width // 2, final_image.height // 2))\n",
    "\n",
    "        output_filename = os.path.join(output_folder, f\"stitched_{i+1:04d}.png\")\n",
    "        final_image.save(output_filename)\n",
    "\n",
    "# Example usage\n",
    "# stitch_and_save_image_sequence_varying_sizes('path/to/master_folder', 'path/to/output_folder', reduce_size=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master='D://Michael_Shannon/FUNCTIONTEST/'\n",
    "output='D://Michael_Shannon/FUNCTIONTEST_output3/'\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "stitch_and_save_image_sequence_varying_sizes(master, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'D://Michael_Shannon/CELLPLATO_MASTER/COMBO_Donor2_Donor3_Donor7_OUTPUT/Donor2_3_and7_renamedto4/2024-03-20_21-55-01-520669/plots/Clustering/Trajectory_Cluster_Disambiguation/'\n",
    "\n",
    "# folder_path='D:\\Michael_Shannon\\Trajectory_Cluster_Disambiguation'\n",
    "\n",
    "# folder_path='D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCD_VERSION1\\Trajectory_Cluster_DisambiguationABCD\\TRAJECTORY'\n",
    "# folder_path='D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCD_VERSION2'\n",
    "folder_path='D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCDVERSION3_150\\Trajectory_Cluster_Disambiguation'\n",
    "\n",
    "for foldername in os.listdir(folder_path):\n",
    "    print(foldername)\n",
    "    master = os.path.join(folder_path, foldername)\n",
    "    print(master)\n",
    "\n",
    "    # add the suffix _output onto the end of master\n",
    "    output = master + '_output'\n",
    "    print(output)\n",
    "    # clear the folder_path variable before the next loop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Example usage\n",
    "    stitch_and_save_image_sequence_varying_sizes(master, output, reduce_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# folder_path = 'D://Michael_Shannon/CELLPLATO_MASTER/COMBO_Donor2_Donor3_Donor7_OUTPUT/Donor2_3_and7_renamedto4/2024-03-20_21-55-01-520669/plots/Clustering/Trajectory_Cluster_Disambiguation/'\n",
    "\n",
    "# folder_path='D:\\Michael_Shannon\\Trajectory_Cluster_Disambiguation'\n",
    "\n",
    "# folder_path='D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCD_VERSION1\\Trajectory_Cluster_DisambiguationABCD\\TRAJECTORY'\n",
    "# folder_path='D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCD_VERSION2'\n",
    "folder_path = 'D:\\Michael_Shannon\\CELLPLATO_MASTER\\JCS_ReviewersComments_2-2024\\edited\\VERSION_DONOR1_DONORS2_and_4\\ABCDVERSION3_150\\Trajectory_Cluster_Disambiguation'\n",
    "\n",
    "for foldername in os.listdir(folder_path):\n",
    "    if os.path.isdir(os.path.join(folder_path, foldername)):\n",
    "        print(foldername)\n",
    "        master = os.path.join(folder_path, foldername)\n",
    "        print(master)\n",
    "\n",
    "        # add the suffix _output onto the end of master\n",
    "        output = master + '_output'\n",
    "        print(output)\n",
    "        # clear the folder_path variable before the next loop\n",
    "\n",
    "        # Example usage\n",
    "        stitch_and_save_image_sequence_varying_sizes(master, output, reduce_size=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Things if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=cp.plots_of_differences_donors(tavg_df,factor=chosen_factor)\n",
    "f.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellPLATO_gitversion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
