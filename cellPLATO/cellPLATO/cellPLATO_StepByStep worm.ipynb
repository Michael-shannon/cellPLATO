{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (worm)CellPLATO | Cell Plasticity Analysis Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3207632024.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\macelabGPU\\AppData\\Local\\Temp\\ipykernel_5648\\3207632024.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    'cellinf' is a json file containing information about the cells, their names, lineage, cell type etc.\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'cellinf' is a json file containing information about the cells, their names, lineage, cell type etc. \n",
    "\n",
    "'gene_CPM' is cell x gene x time matrix\n",
    "\n",
    "'gene_names' is a list of the corresponding gene names\n",
    "\n",
    "'gene_time' the time sampling for gene and position data is different. This vector contains the timestamps for the gene data. \n",
    "\n",
    "'pos_rpz' 3 x cell x time matrix  containing the cell positions in the cylindrical coordinate system. r is radial distance, p is the angular distance and z is the distance along the A-P axis.\n",
    "\n",
    "'pos_time' timestamp for the position data. \n",
    "\n",
    "'pos_xyz' 3 x cell x time matrix with the cartesian position of each cell over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fill in the config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Set your kernel to 'cellPLATO' before continuing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>1. Start by importing packages for cellPLATO</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes cellPLATO itself, and all of the modules you will need\n",
    "\n",
    "* Import these packages, checking that you have them\n",
    "* We're also importing a lot of the modules in cellPLATO, if this cell runs successfully, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running cellPLATO initialization and loaded config.\n",
      "Initializing:  WormData\n",
      "Hypthesis testing using:  st.ttest_ind\n",
      "Plots will be exported to:  D:/Michael_Shannon/Hari_Collab/test_data_OUTPUT/WormData\\2024-02-21_17-11-10-644365\\plots/\n",
      "Using unique embedding per dataset shortname:  WormData\n",
      "Exporting static Superplots\n",
      "Exporting static Superplots\n",
      "Exporting static Plots of Differences\n",
      "Exporting static Marginal scatterplots\n",
      "Exporting static Timeplots\n",
      "Exporting Bar plots\n",
      "Exporting SNS Bar plots\n",
      "Using corresponding CTL_SHORTLABEL:  Worm_one  for condition:  Condition_one\n",
      "Dataset in current notebook:  WormData\n",
      "Finished initializing data_processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file c:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished intializing visualizations\n",
      "Finished initializing cellPLATO\n"
     ]
    }
   ],
   "source": [
    "import cellPLATO as cp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "OVERWRITE_DATAFRAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['cellinf', 'gene_CPM', 'gene_names', 'gene_time', 'pos_rpz', 'pos_time', 'pos_xyz']>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Iterate over the folders in the master directory\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    \n",
    "    # Check if the folder is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Iterate over the files in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            # Check if the file is an h5 file\n",
    "            if file.endswith('.h5'):\n",
    "                # Open the h5 file\n",
    "                with h5py.File(file_path, 'r') as f:\n",
    "                    # Print the keys of the h5 file\n",
    "                    print(f.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'cellinf' is a json file containing information about the cells, their names, lineage, cell type etc. \n",
    "\n",
    "'gene_CPM' is cell x gene x time matrix\n",
    "\n",
    "'gene_names' is a list of the corresponding gene names\n",
    "\n",
    "'gene_time' the time sampling for gene and position data is different. This vector contains the timestamps for the gene data. \n",
    "\n",
    "'pos_rpz' 3 x cell x time matrix  containing the cell positions in the cylindrical coordinate system. r is radial distance, p is the angular distance and z is the distance along the A-P axis.\n",
    "\n",
    "'pos_time' timestamp for the position data. \n",
    "\n",
    "'pos_xyz' 3 x cell x time matrix with the cartesian position of each cell over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0: This is a check cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pos_rpz: (331, 321, 3)\n",
      "Sample data (first cell, first time point):\n",
      "r (radial distance): 3.092721700668335\n",
      "p (angular distance): 3.0966713428497314\n",
      "z (distance along A-P axis): 3.096306324005127\n",
      "Warning: The first dimension does not match the expected size for r, p, z.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Placeholder for the last read h5 file path\n",
    "h5_file_path = None\n",
    "\n",
    "# Iterate and find the last h5 file for demonstration\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    # Load the dataset\n",
    "    pos_rpz = f['pos_rpz'][()]\n",
    "    \n",
    "    # 1. Check the shape\n",
    "    print(\"Shape of pos_rpz:\", pos_rpz.shape)\n",
    "    \n",
    "    # The shape should be (3, num_cells, num_times)\n",
    "    \n",
    "    # 2. Inspect the data\n",
    "    # Viewing a small portion to understand its arrangement\n",
    "    # For example, let's view the first cell's data at the first time point\n",
    "    print(\"Sample data (first cell, first time point):\")\n",
    "    print(\"r (radial distance):\", pos_rpz[0, 0, 0])\n",
    "    print(\"p (angular distance):\", pos_rpz[1, 0, 0])\n",
    "    print(\"z (distance along A-P axis):\", pos_rpz[2, 0, 0])\n",
    "    \n",
    "    # Verify that the first dimension indeed corresponds to r, p, z for a sanity check\n",
    "    if pos_rpz.shape[0] != 3:\n",
    "        print(\"Warning: The first dimension does not match the expected size for r, p, z.\")\n",
    "    else:\n",
    "        print(\"The first dimension correctly represents r, p, z coordinates.\")\n",
    "    \n",
    "    # Additional checks can be done here depending on what specific issues you're concerned about\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: load everything from the h5, doing a special JSON load for the cellinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lineage_id': 'ABpraappaa', 'name': 'hyp4', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABplaappaa', 'name': 'hyp4', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABplaaapaa', 'name': 'XXXL', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABarpappaa', 'name': 'XXXR', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABarapaaapa', 'name': 'm1VR', 'type': 'muscle', 'cell_death': False, 'isterminal': True}]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Placeholder for the last read h5 file path\n",
    "h5_file_path = None\n",
    "\n",
    "# Iterate and find the last h5 file for demonstration\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    # Load the datasets\n",
    "    pos_rpz = f['pos_rpz'][()]\n",
    "    # cellinf = f['cellinf'][()]\n",
    "    gene_CPM = f['gene_CPM'][()]\n",
    "    gene_names = f['gene_names'][()] # time, gene expression level, cell\n",
    "    gene_time = f['gene_time'][()] # A vector of 130 elements which is the REAL time sampling for the gene data\n",
    "    pos_xyz = f['pos_xyz'][()]\n",
    "    pos_time = f['pos_time'][()] # A vector of 331 elements which is the REAL time sampling for the position data.\n",
    "\n",
    "\n",
    "    # This is special because it is a byte string from a JSON object\n",
    "\n",
    "    if 'cellinf' in f:\n",
    "        # Extract the dataset\n",
    "        json_data_array = f['cellinf'][()]\n",
    "\n",
    "        # Given the shape is (1, 1), access the nested byte string\n",
    "        if json_data_array.size > 0:\n",
    "            json_bytes = json_data_array[0, 0]  # Access the byte string\n",
    "\n",
    "            # Decode the byte string to get the JSON string\n",
    "            json_str = json_bytes.decode('utf-8')\n",
    "            \n",
    "            # Parse the JSON string into a Python object\n",
    "            cell_info = json.loads(json_str)\n",
    "            \n",
    "            # Print the first few items to check\n",
    "            print(cell_info[:5])\n",
    "        else:\n",
    "            print(\"The 'cellinf' dataset appears to be empty.\")\n",
    "    else:\n",
    "        print(\"'cellinf' dataset does not exist in the HDF5 file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: make the dataframe from the radial coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          r         p          z  cell_id  time_id\n",
      "0  3.092722 -1.208199  15.028050        0        0\n",
      "1  3.148723 -1.970752  15.410750        0        1\n",
      "2  4.094421 -2.825229  16.787140        0        2\n",
      "3  3.943589 -0.301919  18.441759        0        3\n",
      "4  2.134434 -2.099260  21.032261        0        4\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Placeholder for the last read h5 file path\n",
    "h5_file_path = None\n",
    "\n",
    "# Iterate and find the last h5 file for demonstration\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "if h5_file_path:\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        pos_rpz = f['pos_rpz'][()]  # Assuming shape is (3, num_cells, num_times)\n",
    "\n",
    "    # Since the last dimension is the coordinates, we can reshape directly\n",
    "    num_cells, num_times = pos_rpz.shape[0], pos_rpz.shape[1]\n",
    "    # Flatten the array while keeping the r, p, z coordinates intact\n",
    "    data_reshaped = pos_rpz.reshape(num_cells * num_times, 3)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_pos_rpz = pd.DataFrame(data_reshaped, columns=['r', 'p', 'z'])\n",
    "\n",
    "    # Generate cell_id and time_id\n",
    "    cell_ids = np.repeat(np.arange(num_cells), num_times)\n",
    "    time_ids = np.tile(np.arange(num_times), num_cells)\n",
    "\n",
    "    # Add cell_id and time_id to the DataFrame\n",
    "    df_pos_rpz['cell_id'] = cell_ids\n",
    "    df_pos_rpz['time_id'] = time_ids\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df_pos_rpz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Map the contents of cell_info to the dataframe and add them in (cell names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          r         p          z  cell_id  time_id  lineage_id cell_type  \\\n",
      "0  3.092722 -1.208199  15.028050        0        0  ABpraappaa  hypoderm   \n",
      "1  3.148723 -1.970752  15.410750        0        1  ABpraappaa  hypoderm   \n",
      "2  4.094421 -2.825229  16.787140        0        2  ABpraappaa  hypoderm   \n",
      "3  3.943589 -0.301919  18.441759        0        3  ABpraappaa  hypoderm   \n",
      "4  2.134434 -2.099260  21.032261        0        4  ABpraappaa  hypoderm   \n",
      "\n",
      "  cell_death isterminal  \n",
      "0      False       True  \n",
      "1      False       True  \n",
      "2      False       True  \n",
      "3      False       True  \n",
      "4      False       True  \n"
     ]
    }
   ],
   "source": [
    "########################## ADDING THE JSON INFO TO THE DATAFRAME ##############################\n",
    "\n",
    "# Assuming cell_info and df_pos_rpz are already defined and populated as before\n",
    "\n",
    "# Extend the cell_name_mapping to include additional attributes\n",
    "cell_attributes_mapping = {i: {'name': cell['name'], \n",
    "                               'lineage_id': cell['lineage_id'], \n",
    "                               'type': cell['type'], \n",
    "                               'cell_death': cell['cell_death'], \n",
    "                               'isterminal': cell['isterminal']} \n",
    "                           for i, cell in enumerate(cell_info)}\n",
    "\n",
    "# Function to safely get attributes for each cell_id\n",
    "def get_cell_attribute(cell_id, attribute):\n",
    "    return cell_attributes_mapping.get(cell_id, {}).get(attribute, 'Unknown')\n",
    "\n",
    "# Now, map each cell_id in your DataFrame to its corresponding attributes\n",
    "df_pos_rpz['lineage_id'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'lineage_id'))\n",
    "df_pos_rpz['cell_type'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'type'))\n",
    "df_pos_rpz['cell_death'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'cell_death'))\n",
    "df_pos_rpz['isterminal'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'isterminal'))\n",
    "\n",
    "# Now, df_pos_rpz includes additional columns for lineage_id, cell_type, cell_death, and isterminal\n",
    "print(df_pos_rpz.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>z</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>lineage_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_death</th>\n",
       "      <th>isterminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.092722</td>\n",
       "      <td>-1.208199e+00</td>\n",
       "      <td>15.028050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ABpraappaa</td>\n",
       "      <td>hypoderm</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.148723</td>\n",
       "      <td>-1.970752e+00</td>\n",
       "      <td>15.410750</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ABpraappaa</td>\n",
       "      <td>hypoderm</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.094421</td>\n",
       "      <td>-2.825229e+00</td>\n",
       "      <td>16.787140</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>ABpraappaa</td>\n",
       "      <td>hypoderm</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.943589</td>\n",
       "      <td>-3.019188e-01</td>\n",
       "      <td>18.441759</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>ABpraappaa</td>\n",
       "      <td>hypoderm</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.134434</td>\n",
       "      <td>-2.099260e+00</td>\n",
       "      <td>21.032261</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>ABpraappaa</td>\n",
       "      <td>hypoderm</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106246</th>\n",
       "      <td>1.976188</td>\n",
       "      <td>-5.053166e-16</td>\n",
       "      <td>184.381393</td>\n",
       "      <td>330</td>\n",
       "      <td>316</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106247</th>\n",
       "      <td>1.536988</td>\n",
       "      <td>1.591760e+00</td>\n",
       "      <td>185.985107</td>\n",
       "      <td>330</td>\n",
       "      <td>317</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106248</th>\n",
       "      <td>0.902613</td>\n",
       "      <td>-1.439479e+00</td>\n",
       "      <td>189.487793</td>\n",
       "      <td>330</td>\n",
       "      <td>318</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106249</th>\n",
       "      <td>0.538817</td>\n",
       "      <td>5.551305e-01</td>\n",
       "      <td>185.632797</td>\n",
       "      <td>330</td>\n",
       "      <td>319</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106250</th>\n",
       "      <td>0.502632</td>\n",
       "      <td>-1.298370e+00</td>\n",
       "      <td>192.828201</td>\n",
       "      <td>330</td>\n",
       "      <td>320</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106251 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               r             p           z  cell_id  time_id  lineage_id  \\\n",
       "0       3.092722 -1.208199e+00   15.028050        0        0  ABpraappaa   \n",
       "1       3.148723 -1.970752e+00   15.410750        0        1  ABpraappaa   \n",
       "2       4.094421 -2.825229e+00   16.787140        0        2  ABpraappaa   \n",
       "3       3.943589 -3.019188e-01   18.441759        0        3  ABpraappaa   \n",
       "4       2.134434 -2.099260e+00   21.032261        0        4  ABpraappaa   \n",
       "...          ...           ...         ...      ...      ...         ...   \n",
       "106246  1.976188 -5.053166e-16  184.381393      330      316     Unknown   \n",
       "106247  1.536988  1.591760e+00  185.985107      330      317     Unknown   \n",
       "106248  0.902613 -1.439479e+00  189.487793      330      318     Unknown   \n",
       "106249  0.538817  5.551305e-01  185.632797      330      319     Unknown   \n",
       "106250  0.502632 -1.298370e+00  192.828201      330      320     Unknown   \n",
       "\n",
       "       cell_type cell_death isterminal  \n",
       "0       hypoderm      False       True  \n",
       "1       hypoderm      False       True  \n",
       "2       hypoderm      False       True  \n",
       "3       hypoderm      False       True  \n",
       "4       hypoderm      False       True  \n",
       "...          ...        ...        ...  \n",
       "106246   Unknown    Unknown    Unknown  \n",
       "106247   Unknown    Unknown    Unknown  \n",
       "106248   Unknown    Unknown    Unknown  \n",
       "106249   Unknown    Unknown    Unknown  \n",
       "106250   Unknown    Unknown    Unknown  \n",
       "\n",
       "[106251 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_rpz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lineage_id': 'ABpraappaa', 'name': 'hyp4', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABplaappaa', 'name': 'hyp4', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABplaaapaa', 'name': 'XXXL', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABarpappaa', 'name': 'XXXR', 'type': 'hypoderm', 'cell_death': False, 'isterminal': True}, {'lineage_id': 'ABarapaaapa', 'name': 'm1VR', 'type': 'muscle', 'cell_death': False, 'isterminal': True}]\n"
     ]
    }
   ],
   "source": [
    "# # THIS GETS THE JSON STUFF\n",
    "\n",
    "# import h5py\n",
    "# import json\n",
    "\n",
    "# master_directory = cp.DATA_PATH\n",
    "\n",
    "# # Placeholder for the last read h5 file path\n",
    "# h5_file_path = None\n",
    "\n",
    "# # Iterate and find the last h5 file for demonstration\n",
    "# for folder in os.listdir(master_directory):\n",
    "#     folder_path = os.path.join(master_directory, folder)\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         for file in os.listdir(folder_path):\n",
    "#             h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "\n",
    "\n",
    "# with h5py.File(h5_file_path, 'r') as f:\n",
    "#     # Check if 'cellinf' exists in the file\n",
    "#     if 'cellinf' in f:\n",
    "#         # Extract the dataset\n",
    "#         json_data_array = f['cellinf'][()]\n",
    "\n",
    "#         # Given the shape is (1, 1), access the nested byte string\n",
    "#         if json_data_array.size > 0:\n",
    "#             json_bytes = json_data_array[0, 0]  # Access the byte string\n",
    "\n",
    "#             # Decode the byte string to get the JSON string\n",
    "#             json_str = json_bytes.decode('utf-8')\n",
    "            \n",
    "#             # Parse the JSON string into a Python object\n",
    "#             cell_info = json.loads(json_str)\n",
    "            \n",
    "#             # Print the first few items to check\n",
    "#             print(cell_info[:5])\n",
    "#         else:\n",
    "#             print(\"The 'cellinf' dataset appears to be empty.\")\n",
    "#     else:\n",
    "#         print(\"'cellinf' dataset does not exist in the HDF5 file.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below illustrates that the times don't match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[465. 470. 475. 480. 485. 490. 495. 500. 505. 510. 515. 520. 525. 530.\n",
      " 535. 540. 545. 550. 555. 560. 565. 570. 575. 580. 585. 595. 600. 610.\n",
      " 615. 620. 630. 635. 640. 650. 665. 670. 675. 680. 685. 705. 710. 715.\n",
      " 740. 745. 765. 770. 775.]\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(pos_time)\n",
    "print(gene_time)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "overlap = np.intersect1d(pos_time, gene_time)\n",
    "print(overlap)\n",
    "print(len(overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          r         p          z  cell_id  time_id\n",
      "0  3.092722 -1.208199  15.028050        0        0\n",
      "1  3.148723 -1.970752  15.410750        0        1\n",
      "2  4.094421 -2.825229  16.787140        0        2\n",
      "3  3.943589 -0.301919  18.441759        0        3\n",
      "4  2.134434 -2.099260  21.032261        0        4\n"
     ]
    }
   ],
   "source": [
    "############################ KEEEEEEEEEEEEEEEEEEEEEPPPP #####################\n",
    "\n",
    "\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Placeholder for the last read h5 file path\n",
    "h5_file_path = None\n",
    "\n",
    "# Iterate and find the last h5 file for demonstration\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "if h5_file_path:\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        pos_rpz = f['pos_rpz'][()]  # Assuming shape is (3, num_cells, num_times)\n",
    "\n",
    "    # Since the last dimension is the coordinates, we can reshape directly\n",
    "    num_cells, num_times = pos_rpz.shape[0], pos_rpz.shape[1]\n",
    "    # Flatten the array while keeping the r, p, z coordinates intact\n",
    "    data_reshaped = pos_rpz.reshape(num_cells * num_times, 3)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_pos_rpz = pd.DataFrame(data_reshaped, columns=['r', 'p', 'z'])\n",
    "\n",
    "    # Generate cell_id and time_id\n",
    "    cell_ids = np.repeat(np.arange(num_cells), num_times)\n",
    "    time_ids = np.tile(np.arange(num_times), num_cells)\n",
    "\n",
    "    # Add cell_id and time_id to the DataFrame\n",
    "    df_pos_rpz['cell_id'] = cell_ids\n",
    "    df_pos_rpz['time_id'] = time_ids\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df_pos_rpz.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          r         p          z  cell_id  time_id  lineage_id cell_type  \\\n",
      "0  3.092722 -1.208199  15.028050        0        0  ABpraappaa  hypoderm   \n",
      "1  3.148723 -1.970752  15.410750        0        1  ABpraappaa  hypoderm   \n",
      "2  4.094421 -2.825229  16.787140        0        2  ABpraappaa  hypoderm   \n",
      "3  3.943589 -0.301919  18.441759        0        3  ABpraappaa  hypoderm   \n",
      "4  2.134434 -2.099260  21.032261        0        4  ABpraappaa  hypoderm   \n",
      "\n",
      "  cell_death isterminal  \n",
      "0      False       True  \n",
      "1      False       True  \n",
      "2      False       True  \n",
      "3      False       True  \n",
      "4      False       True  \n"
     ]
    }
   ],
   "source": [
    "########################## ADDING THE JSON INFO TO THE DATAFRAME ##############################\n",
    "\n",
    "# Assuming cell_info and df_pos_rpz are already defined and populated as before\n",
    "\n",
    "# Extend the cell_name_mapping to include additional attributes\n",
    "cell_attributes_mapping = {i: {'name': cell['name'], \n",
    "                               'lineage_id': cell['lineage_id'], \n",
    "                               'type': cell['type'], \n",
    "                               'cell_death': cell['cell_death'], \n",
    "                               'isterminal': cell['isterminal']} \n",
    "                           for i, cell in enumerate(cell_info)}\n",
    "\n",
    "# Function to safely get attributes for each cell_id\n",
    "def get_cell_attribute(cell_id, attribute):\n",
    "    return cell_attributes_mapping.get(cell_id, {}).get(attribute, 'Unknown')\n",
    "\n",
    "# Now, map each cell_id in your DataFrame to its corresponding attributes\n",
    "df_pos_rpz['lineage_id'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'lineage_id'))\n",
    "df_pos_rpz['cell_type'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'type'))\n",
    "df_pos_rpz['cell_death'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'cell_death'))\n",
    "df_pos_rpz['isterminal'] = df_pos_rpz['cell_id'].apply(lambda x: get_cell_attribute(x, 'isterminal'))\n",
    "\n",
    "# Now, df_pos_rpz includes additional columns for lineage_id, cell_type, cell_death, and isterminal\n",
    "print(df_pos_rpz.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          r         p          z  cell_id  time_id cell_name\n",
      "0  3.092722 -1.208199  15.028050        0        0      hyp4\n",
      "1  3.148723 -1.970752  15.410750        0        1      hyp4\n",
      "2  4.094421 -2.825229  16.787140        0        2      hyp4\n",
      "3  3.943589 -0.301919  18.441759        0        3      hyp4\n",
      "4  2.134434 -2.099260  21.032261        0        4      hyp4\n"
     ]
    }
   ],
   "source": [
    "# Assuming cell_info and df_pos_rpz are already defined and populated as before\n",
    "\n",
    "# Create a mapping of cell_ids to cell names\n",
    "# This assumes each cell name in cell_info should correspond to a cell_id in your DataFrame\n",
    "cell_name_mapping = {i: cell['name'] for i, cell in enumerate(cell_info)}\n",
    "\n",
    "# Now, map each cell_id in your DataFrame to its corresponding cell name\n",
    "# If a cell_id does not have a corresponding entry in cell_name_mapping, use 'Unknown' or another placeholder\n",
    "df_pos_rpz['cell_name'] = df_pos_rpz['cell_id'].apply(lambda x: cell_name_mapping.get(x, 'Unknown'))\n",
    "\n",
    "# Now, df_pos_rpz includes a cell_name column\n",
    "print(df_pos_rpz.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known cell names: 103041\n",
      "Number of unknown cell names: 3210\n"
     ]
    }
   ],
   "source": [
    "# How many unknowns are there compared to known cell names?\n",
    "num_unknowns = df_pos_rpz['cell_name'].value_counts().get('Unknown', 0)\n",
    "num_knowns = df_pos_rpz.shape[0] - num_unknowns\n",
    "print(f\"Number of known cell names: {num_knowns}\")\n",
    "print(f\"Number of unknown cell names: {num_unknowns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cells: 331\n"
     ]
    }
   ],
   "source": [
    "# how many unique cell ids are there in df_pos_rpz\n",
    "num_unique_cells = df_pos_rpz['cell_id'].nunique()\n",
    "print(\"Number of unique cells:\", num_unique_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (103041) does not match length of index (106251)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5648\\3258049774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mdf_pos_rpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cell_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mdf_pos_rpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mdf_pos_rpz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cell_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextended_cell_names\u001b[0m  \u001b[1;31m# Add cell names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# Display the first few rows of the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3162\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3240\u001b[0m         \"\"\"\n\u001b[0;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3242\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3898\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3899\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3901\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cellPLATO_gitversion\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         raise ValueError(\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[1;34m\"does not match length of index \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (103041) does not match length of index (106251)"
     ]
    }
   ],
   "source": [
    "##### THIS FROM GPT #####\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "# Placeholder for the last read h5 file path\n",
    "h5_file_path = None\n",
    "\n",
    "# Iterate and find the last h5 file for demonstration\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "if h5_file_path:\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        pos_rpz = f['pos_rpz'][()]  # Assuming shape is (num_cells, num_times)\n",
    "        # Extract cellinf JSON data\n",
    "        cellinf_json = f['cellinf'][()][0][0].decode('utf-8')  # Adjusted based on structure\n",
    "        cell_info = json.loads(cellinf_json)\n",
    "\n",
    "    # Since the last dimension is the coordinates, we can reshape directly\n",
    "    num_cells, num_times = pos_rpz.shape[0], pos_rpz.shape[1]\n",
    "    # Flatten the array while keeping the r, p, z coordinates intact\n",
    "    data_reshaped = pos_rpz.reshape(num_cells * num_times, 3)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_pos_rpz = pd.DataFrame(data_reshaped, columns=['r', 'p', 'z'])\n",
    "\n",
    "    # Generate cell_id and time_id\n",
    "    cell_ids = np.repeat(np.arange(num_cells), num_times)\n",
    "    time_ids = np.tile(np.arange(num_times), num_cells)\n",
    "\n",
    "    # Map cell IDs to cell names\n",
    "    cell_names = [cell['name'] for cell in cell_info]  # Assume cell_info is a list of dicts with 'name'\n",
    "    # Extend cell names to match the length of cell_ids in the DataFrame\n",
    "    extended_cell_names = np.repeat(cell_names, num_times)\n",
    "\n",
    "    # Add cell_id, time_id, and cell names to the DataFrame\n",
    "    df_pos_rpz['cell_id'] = cell_ids\n",
    "    df_pos_rpz['time_id'] = time_ids\n",
    "    df_pos_rpz['cell_name'] = extended_cell_names  # Add cell names\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df_pos_rpz.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             r             p           z  cell_id  time_id\n",
      "1605  3.059645 -1.232320e+00   15.193180        5        0\n",
      "1606  3.064366 -1.940671e+00   15.495850        5        1\n",
      "1607  4.152063 -2.815799e+00   17.506491        5        2\n",
      "1608  3.916120 -3.043591e-01   18.507231        5        3\n",
      "1609  2.030680 -2.124073e+00   20.972900        5        4\n",
      "...        ...           ...         ...      ...      ...\n",
      "1921  2.518113 -4.957089e-17  138.542496        5      316\n",
      "1922  1.722124  1.846520e+00  138.499207        5      317\n",
      "1923  1.167557 -1.557348e+00  139.856995        5      318\n",
      "1924  1.429230  1.134767e+00  140.308594        5      319\n",
      "1925  0.602170 -1.673056e+00  142.803299        5      320\n",
      "\n",
      "[321 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>max_time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>326</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>327</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>328</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>330</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cell_id  max_time_id\n",
       "0          0          320\n",
       "1          1          320\n",
       "2          2          320\n",
       "3          3          320\n",
       "4          4          320\n",
       "..       ...          ...\n",
       "326      326          320\n",
       "327      327          320\n",
       "328      328          320\n",
       "329      329          320\n",
       "330      330          320\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract an example cell_id from that data\n",
    "cell_id = 5\n",
    "# extract a dataframe containing only that cell\n",
    "df_cell = df_pos_rpz[df_pos_rpz['cell_id'] == cell_id]\n",
    "# display iy\n",
    "print(df_cell)\n",
    "\n",
    "# Plot the max time id for each cell\n",
    "df_max_time = df_pos_rpz.groupby('cell_id')['time_id'].max().reset_index()\n",
    "df_max_time.columns = ['cell_id', 'max_time_id']\n",
    "df_max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  3.0927217   -1.20819914  15.02805042]\n",
      "  [  3.14872289  -1.970752    15.41075039]\n",
      "  [  4.09442139  -2.82522869  16.78713989]\n",
      "  ...\n",
      "  [  1.28960478  -1.46278107 136.0007019 ]\n",
      "  [  1.49707925   1.14948523 136.72140503]\n",
      "  [  0.66882074  -1.56147957 138.78819275]]\n",
      "\n",
      " [[  3.09667134  -1.21407139  15.05492973]\n",
      "  [  3.14045668  -1.96326554  15.42298031]\n",
      "  [  4.10371256  -2.81815958  16.90217972]\n",
      "  ...\n",
      "  [  1.2685194   -1.48638749 136.73500061]\n",
      "  [  1.48197532   1.14719629 137.43559265]\n",
      "  [  0.67123091  -1.58409417 139.59829712]]\n",
      "\n",
      " [[  3.09630632  -1.21882415  15.08452988]\n",
      "  [  3.12669277  -1.9559716   15.43801022]\n",
      "  [  4.11432838  -2.81393862  17.03248024]\n",
      "  ...\n",
      "  [  1.24445486  -1.50697112 137.49749756]\n",
      "  [  1.46793652   1.14435792 138.15409851]\n",
      "  [  0.66521484  -1.60240138 140.41499329]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  2.43339562  -1.61188483  19.32139969]\n",
      "  [  2.43172789  -1.61731923  19.75414085]\n",
      "  [  5.09780216  -2.8565743   44.52928162]\n",
      "  ...\n",
      "  [  0.9287312   -1.43783271 189.26359558]\n",
      "  [  0.54483759   0.56692207 185.22109985]\n",
      "  [  0.52307779  -1.26306605 192.46699524]]\n",
      "\n",
      " [[  2.43673682  -1.61119092  19.28877068]\n",
      "  [  2.44590926  -1.61808515  19.71808052]\n",
      "  [  5.1016984   -2.85550332  44.51538849]\n",
      "  ...\n",
      "  [  0.9135893   -1.43972838 189.3697052 ]\n",
      "  [  0.54084384   0.5613606  185.42720032]\n",
      "  [  0.511989    -1.2787317  192.63580322]]\n",
      "\n",
      " [[  2.43765783  -1.61238468  19.25938988]\n",
      "  [  2.45328808  -1.61903751  19.68712997]\n",
      "  [  5.1046257   -2.85413051  44.50032043]\n",
      "  ...\n",
      "  [  0.90261257  -1.43947935 189.48779297]\n",
      "  [  0.53881669   0.55513048 185.63279724]\n",
      "  [  0.50263155  -1.29837036 192.82820129]]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "master_directory = cp.DATA_PATH\n",
    "\n",
    "#  Iterate over the folders in the master directory\n",
    "for folder in os.listdir(master_directory):\n",
    "    folder_path = os.path.join(master_directory, folder)\n",
    "    \n",
    "    # Check if the folder is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Iterate over the files in the folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            h5_file_path = os.path.join(folder_path, file)\n",
    "\n",
    "\n",
    "# Open the h5 file\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    # Extract the pos_rpz dataset\n",
    "    pos_rpz = f['pos_rpz'][()]\n",
    "\n",
    "\n",
    "# Display a preview of pos_rpz\n",
    "print(pos_rpz)\n",
    "\n",
    "# \n",
    "\n",
    "#    \n",
    "\n",
    "# # Convert pos_rpz to a pandas dataframe\n",
    "# df_pos_rpz = pd.DataFrame(pos_rpz)\n",
    "\n",
    "# # Display the dataframe\n",
    "# print(df_pos_rpz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your experiment list\n",
    "\n",
    "Check that the list generated in the next cell contains your conditions and replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'exp_list_df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21556\\1209023933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the experiment list from the experiments listed in the config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexp_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate_experiment_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_DATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\githubsoftware\\cellPLATO_gitversion_2023\\cellPLATO\\cellPLATO\\cellPLATO\\data_processing\\data_io.py\u001b[0m in \u001b[0;36mpopulate_experiment_list\u001b[1;34m(fmt, save)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# A test to be sure the same Experiment name is not used twice.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexp_list_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Experiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mrep_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_list_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexp_list_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Experiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrep_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Condition'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Experiment name not unique to condition'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'exp_list_df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Get the experiment list from the experiments listed in the config \n",
    "exp_list = cp.populate_experiment_list()\n",
    "display(exp_list)\n",
    "print(cp.SAVED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>2. Measurements of morphology and migration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell does migration and morphology measurements for all of the cells at each timepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, process and combine the dataframes (including segmentation and migration calculations)\n",
    "comb_df = cp.combine_dataframes(exp_list)\n",
    "\n",
    "comb_df, new_factors = cp.measurement_pipeline(comb_df, mixed=cp.MIXED_SCALING, factors_to_timeaverage = cp.ALL_FACTORS) \n",
    "display(new_factors)\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "comb_df, filt_counts = cp.apply_filters(comb_df)\n",
    "\n",
    "# Process a time-averaged DataFrame\n",
    "tavg_df = cp.time_average(comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filt_df or comb_df depending on what you want to see\n",
    "f=cp.multi_condition_timeplot(filt_df, chosen_factor)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tavg_df from csv\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "# Region property factors to be extracted from the cell contours\n",
    "# This list must match with props from regionprops\n",
    "\n",
    "REGIONPROPS_LIST = ['area',\n",
    "                    'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    'orientation',\n",
    "                    'perimeter',\n",
    "                     'solidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=cp.plots_of_differences_sns(tavg_df,factor='rip_L')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then gives you the option to save the dataframes into your automatically created 'saved data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    comb_df.to_csv(cp.SAVED_DATA_PATH + 'comb_df.csv', index=False)\n",
    "    tavg_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load in precreated dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.read_csv(cp.SAVED_DATA_PATH + 'comb_df.csv')\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: do filtering on the data (on top of what has been stated in the config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined filters in dict {factor:(min, max)}\n",
    "\n",
    "data_filters = {\n",
    "#   \"speed\": (10, 100),\n",
    "  \"area\": (50, 10000),\n",
    "#    \"frame\": (0, 450), # Warning: range will change if self-normalized\n",
    "#   \"ntpts\": (12,1800)\n",
    "}\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "filt_df, filt_counts = cp.apply_filters(comb_df,how='any', filter_dict=data_filters)\n",
    "\n",
    "fig = cp.visualize_filtering(filt_df, filt_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a preview plot of any of these factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_factor = 'area' # Enter in a factor from the list below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Factor list: \n",
    "all_factors =  ['area',\n",
    "                'bbox_area',\n",
    "                'eccentricity',\n",
    "                'equivalent_diameter',\n",
    "                'extent',\n",
    "                'filled_area',\n",
    "                'major_axis_length',\n",
    "                'minor_axis_length',\n",
    "                'orientation',\n",
    "                'perimeter',\n",
    "                'solidity',\n",
    "                'euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient'\n",
    "                'aspect',\n",
    "                'rip_p',\n",
    "                'rip_K',\n",
    "                'rip_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filt_df or comb_df depending on what you want to see\n",
    "f=cp.multi_condition_timeplot(filt_df, chosen_factor)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all metrics\n",
    "\n",
    "This cell makes comparative plots for every single metric and saves them in your output folder\n",
    "\n",
    "* Plots of difference\n",
    "* Timeplots of difference\n",
    "* Marginal xy plots\n",
    "* Simple bar plots\n",
    "* Superplots - useful for comparing between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Check that you are happy with your extra filtering before continuing\n",
    "Run the next cell on the filtered dataframe or the unfiltered dataframe once you are ready\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs plots of all metrics for all factors\n",
    "cp.comparative_visualization_pipeline(comb_df, num_factors=all_factors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>3. Definition of single timepoint behavioural clusters using UMAP and HDBSCAN</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: for new datasets perform correlation analysis to understand which factors correlate to one another\n",
    "\n",
    "This may aid in choosing the most important factors, aiding clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = comb_df\n",
    "cp.correlation_matrix_heatmap(df_in, factors = cp.ALL_FACTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: use variance thresholder for further insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dr_factors = cp.variance_threshold(comb_df, threshold_value=0.03)\n",
    "chosen_dr_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: define a new list of dr_factors to use for UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONPROPS_LIST = ['area',\n",
    "                    'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    'orientation',\n",
    "                    'perimeter',\n",
    "                     'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                'orientedness', \n",
    "                'directedness',\n",
    "                'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform UMAP and cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, do UMAP, save the new df and plot the UMAP\n",
    "\n",
    "Well separated clusters depend mostly on 1. the input factors and 2. the umap_nn setting\n",
    "\n",
    "You can change both, depending on the nature of your data, in order to achieve a reasonable level of separation of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### User alterable parameters ######\n",
    "tsne_perp=150\n",
    "umap_nn = 30 #umap nearest neighbours\n",
    "min_dist = 0.0 #umap minimum distance (usually keep this at 0 or very low)\n",
    "n_components = 3 # number of umap dimensions to calculate\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "dr_df = cp.dr_pipeline_multiUMAPandTSNE(comb_df, \n",
    "                    dr_factors=DR_FACTORS,\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) # A number of scaling methods are available: 'choice', 'minmax', 'standard', 'robust', 'normalize', 'quantile', 'maxabs', 'yeo-johnson', 'box-cox'\n",
    "\n",
    "dr_df.to_csv(cp.SAVED_DATA_PATH + 'dr_df.csv', index=False) # Saves the df\n",
    "\n",
    "cp.plot_3D_scatter(dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier='dr_df' + '_byCONDITION_',dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, identify clusters and exemplar cells using HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User adjustable parameters #####\n",
    "min_cluster_size = 1000\n",
    "min_samples = 500\n",
    "cluster_by = 'UMAPNDIM' # UMAPNDIM = default, clusters on UMAPs. NDIM = alternate, clusters on all dimensions\n",
    "metric = 'euclidean' # See https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan.HDBSCAN for options\n",
    "#######################################\n",
    "\n",
    "lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_by=cluster_by,  metric=metric)\n",
    "\n",
    "lab_dr_df.name='lab_dr_df'\n",
    "name = lab_dr_df.name\n",
    "\n",
    "lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv', index=False)\n",
    "exemplar_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv', index=False)\n",
    "\n",
    "cp.plot_3D_scatter(lab_dr_df_30, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name + '_byCLUSTERID___',dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp.plot_3D_scatter_dev(exemplar_cell_tracks_df_500_gold, 'UMAP1', 'UMAP2', 'UMAP3', colorby='uniq_id', ticks=False, identifier='exemplar_cell_tracks_df_500_gold' + '_byCONDITION___',dotsize = 20, alpha=0.1, markerscale = 5) #color = label or condition  2\n",
    "# cp.plot_3D_scatter_dev(tptlabel_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='uniq_id', ticks=False, identifier='exemplar_cell_tracks_df_500_gold' + '_byCONDITION___',dotsize = 0.01, alpha=0.1, markerscale = 5) #color = label or condition  2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then plot the 'fingerprint' plot of percentage in each cluster per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new combo\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(lab_dr_df) \n",
    "display(cluster_purity_df)\n",
    "f = cp.purityplot_percentcluspercondition(lab_dr_df, cluster_purity_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: explore the clusters with interactive 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.interactive_plot_3D_UMAP(df=lab_dr_df,colorby = 'Condition_shortlabel', symbolby = 'Condition_shortlabel', what = ' AllTimeUMAPwithclusters') # TavgUMAPwithclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: all other conditions colored grey, chosen condition in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_dr_df\n",
    "\n",
    "condlist = df['Condition_shortlabel'].unique().tolist() #get unique list of conditions from df\n",
    "print(condlist) # show the condition list\n",
    "# chosen_condition = '' #specify a chosen condition from the list\n",
    "chosen_condition = condlist[0] # or choose the first one\n",
    "print(chosen_condition)\n",
    "\n",
    "cp.interactive_plot_3D_UMAP_chosen_condition(df, chosen_condition, opacity_grey=0.01, marker_size_all=2,) #change opacity and marker size to suit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make UMAP plots colored by metric contributors - the more intense the color, the higher the contribution the metric to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one colors per metric\n",
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = False, samplethedf = False)\n",
    "#second one colors per condition\n",
    "# cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=tptlabel_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "#                                                    identifier='inferno', colormap='inferno', coloredbycondition = True, samplethedf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP then HDBSCAN on the tavg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at the moment, just do this step as it is needed for compatibility later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perp=150\n",
    "umap_nn = 20#4#60\n",
    "min_dist = 0.0 #0.15 \n",
    "n_components = 3\n",
    "\n",
    "tavg_dr_df = cp.dr_pipeline_multiUMAPandTSNE(tavg_df, \n",
    "                    dr_factors=new_DR_FACTORS,# new_DR_FACTORS # DR_FACTORS #only_tmeans # cp.DR_FACTORS\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) # log2minmax # powertransformer #minmax\n",
    "\n",
    "lab_tavg_dr_df, exemplar_tavg_df=cp.hdbscan_clustering(tavg_dr_df, min_cluster_size=50,min_samples=50,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Save your dataframes so you can come back to this step if necessary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_dr_df.csv', index=False)\n",
    "    lab_tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv', index=False)\n",
    "    exemplar_tavg_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this function to put the labels into the lab_tavg_lab_dr_df. Slow function. Can update search by uniq_id alone...\n",
    "\n",
    "lab_tavg_lab_dr_df=cp.add_tavglabel_todf(lab_dr_df, lab_tavg_dr_df)\n",
    "lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the plasticity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = cp.count_cluster_changes_with_tavg(lab_tavg_lab_dr_df)\n",
    "tptlabel_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes(df, identifier='\\_allcells', maxy=4) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_countplots(df, identifier='_allcells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_allcells', miny=None, maxy=None, t_window_multiplier = cp.T_WINDOW_MULTIPLIER, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, choose a number of exemplar cells to pick out from the exemplar cell list to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of exemplars to display for each cluster\n",
    "n=2\n",
    "exemplar_df = exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=220 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=3, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, to visualize single cells with many timepoints, select cells with lots of timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User inputs ####\n",
    "whole_df = tptlabel_dr_df\n",
    "exemplar_df = exemplar_df\n",
    "numberofdesiredtimepoints = int(whole_df['ntpts'].mean())\n",
    "# numberofdesiredtimepoints = 200\n",
    "numberofcellspercluster = 40\n",
    "num_clusters_whole_dataset = len(whole_df['label'].unique())\n",
    "\n",
    "override = int((numberofcellspercluster*num_clusters_whole_dataset)*0.7)\n",
    "#####################\n",
    "\n",
    "exemplar_df_filt, exemplar_cell_tracks_df = cp.filter_exemplars(whole_df=whole_df, exemplar_df = exemplar_df, numberofdesiredtimepoints = numberofdesiredtimepoints, \n",
    "                                                                    numberofcellspercluster = numberofcellspercluster, override = override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=exemplar_cell_tracks_df\n",
    "# cp.plot_cumulative_plasticity_changes_test2(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = True) #deprecated, use the small multiples version\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot any factor as small multiples from the exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exemplar_cell_tracks_df\n",
    "whichcolumntoplot = 'label'\n",
    "\n",
    "cp.plot_small_multiples(df, whichcolumntoplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re'disambiguate the new exemplar df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters_topdictionary(df_in=tptlabel_dr_df,  howmanyfactors=10, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint_dev(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONPROPS_LIST = ['area',\n",
    "                    # 'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    # 'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    # 'orientation',\n",
    "                    'perimeter',\n",
    "                    #  'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                # 'orientedness', \n",
    "                # 'directedness',\n",
    "                # 'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                # 'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                # 'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "newnew_DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>4. Trajectory measurement: Damerau-Levenshtein</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter the tptlabel_dr_df to include only a subset of data of similar timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 200\n",
    "high = 220\n",
    "\n",
    "tptlabel_dr_df_filt = tptlabel_dr_df[tptlabel_dr_df['ntpts'].between(low, high)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the filtered data reflects the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorchoice = 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes timeplots of the unfiltered and filtered data\n",
    "\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df, factorchoice)\n",
    "f.show()\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df_filt, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of difference of the unfiltered and filtered data\n",
    "f = cp.plots_of_differences_sns(tavg_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_trajectory_df = cp.time_average(tptlabel_dr_df)\n",
    "f = cp.plots_of_differences_sns(tavg_trajectory_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Damerau-Levenshtein analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt\n",
    "distance_matrix_dameraulev = cp.calculate_edit_distances(df,distancemetric = 'dameraulev', print_interval=10000) #fastdtw # dameraulev # mongeelkan\n",
    "print(distance_matrix_dameraulev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distance matrix\n",
    "# np.save(cp.SAVED_DATA_PATH + 'distance_matrix_dameraulev.npy', distance_matrix_dameraulev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a UMAP/HDBSCAN parameter sweep, and select plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sweep'''\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "for n_neighbors in [8, 10, 12]:\n",
    "    for min_samples in [5,8,10, 15, 30, 40]:\n",
    "        for min_cluster_size in [5,8,10, 15, 30, 40]:\n",
    "            print(f'min_samples = {min_samples}')\n",
    "            print(f'min_cluster_size = {min_cluster_size}')\n",
    "            print(f'n_neighbors = {n_neighbors}')\n",
    "            tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    "             do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Chosen UMAP and HDBSCAN parameters'''\n",
    "\n",
    "min_samples = 30\n",
    "min_cluster_size = 20\n",
    "n_neighbors = 12\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "\n",
    "print(f'min_samples = {min_samples}')\n",
    "print(f'min_cluster_size = {min_cluster_size}')\n",
    "print(f'n_neighbors = {n_neighbors}')\n",
    "tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    " do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fingerprint plot of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(df, cluster_label='trajectory_id') \n",
    "f = cp.purityplot_percentcluspercondition(df, cluster_purity_df, cluster_label='trajectory_id', dotsize = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disambiguate the trajectory clustered cells:\n",
    " 1) Make an exemplar_df_trajectories containing example rows\n",
    " 2) Get the full tracks from those rows and make exemplar_df_trajectories_fulltrack\n",
    " 2) Disambiguate with exemplar_df_trajectories\n",
    " 3) Plot multiples with exemplar_df_trajectories_fulltrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "exemplar_df_trajectories, exemplar_df_trajectories_fulltrack  = cp.make_exemplar_df_basedon_trajectories(df, cells_per_traj=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_tracks_df = pd.read_csv(cp.SAVED_DATA_PATH + 'full_tracks_df.csv')\n",
    "df = exemplar_df_trajectories_fulltrack\n",
    "cp.plot_trajectories(df=exemplar_df_trajectories_fulltrack, global_y=True, global_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "\n",
    "df= tptlabel_dr_df_filt_clusteredtrajectories \n",
    "exemp_df=exemplar_df_trajectories \n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=2, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent fingerprint plot for cluster IDs per TRAJECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-12-2023.csv')\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.fingerprintplot_clusters_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasticity of cells per trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-17-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.plasticity_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes_trajectories(df, identifier='\\_allcells', maxy=9 , t_window_multiplier = 1) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animations of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_trajectory_animations(df, exemplar_df_trajectories, number_of_trajectories=2, colormode='cluster') # singlecluster, cluster, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a number of example cells from each trajectory ID to map back on to the data and display as stacks of PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trajectories = 10 # Select a number of trajectories to plot\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "trajectory_ids = df['trajectory_id'].unique()\n",
    "\n",
    "uniq_id_choices_list = []\n",
    "\n",
    "for trajectory_id_choice in trajectory_ids:\n",
    "    # for each trajectory_id, get a list of possible uniq_ids from the df\n",
    "    uniq_id_choices = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['trajectory_id']==trajectory_id_choice]['uniq_id'].values\n",
    "    # Make sure each once is unique in that list\n",
    "    uniq_id_choices = np.unique(uniq_id_choices)\n",
    "    # choose a number of random uniq_ids from that list based on number_of_trajectories\n",
    "    uniq_id_choices = np.random.choice(uniq_id_choices, number_of_trajectories)\n",
    "    # append each choice to a list\n",
    "    uniq_id_choices_list.append(uniq_id_choices)\n",
    "# flatten the list\n",
    "chosen_uniq_ids = [item for sublist in uniq_id_choices_list for item in sublist]\n",
    "    \n",
    "print(chosen_uniq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_png_behaviour_trajectories(df,chosen_uniq_ids,XYRange = 300, follow_cell = False, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_raw_cell_pngstacks(df,chosen_uniq_ids,XYRange = 220, follow_cell=False, invert=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellPLATO_gitversion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
