{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellPLATO | Cell Plasticity Analysis Tool (Trackmate version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fill in the config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, make sure your data is set up in the following two tiered format:\n",
    "\n",
    "        Master\n",
    "            ├── Condition 1\n",
    "            │   ├── Replicate 1\n",
    "            |   |       ├── tracks.h5\n",
    "            │   ├── Replicate 2\n",
    "            |   |       ├── tracks.h5            \n",
    "            │   └── Replicate 3\n",
    "            |           └── tracks.h5            \n",
    "            │  \n",
    "            └── Condition 2,\n",
    "                ├── Replicate 1\n",
    "                |       ├── tracks.h5\n",
    "                ├── Replicate 2\n",
    "                |       ├── tracks.h5            \n",
    "                └── Replicate 3\n",
    "                        └── tracks.h5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Set your kernel to 'cellPLATO' before continuing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>1. Start by importing packages for cellPLATO</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes cellPLATO itself, and all of the modules you will need\n",
    "\n",
    "* Import these packages, checking that you have them\n",
    "* We're also importing a lot of the modules in cellPLATO, if this cell runs successfully, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellPLATO as cp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import requests\n",
    "\n",
    "\n",
    "OVERWRITE_DATAFRAMES = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your experiment list\n",
    "\n",
    "Check that the list generated in the next cell contains your conditions and replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment list from the experiments listed in the config \n",
    "exp_list = cp.populate_experiment_list()\n",
    "display(exp_list)\n",
    "print(cp.SAVED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the trackmate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spots_df, spots_metadata = cp.load_and_populate(r'.*spots.*\\.csv')\n",
    "\n",
    "merged_tracks_df, tracks_metadata = cp.load_and_populate(r'.*tracks.*\\.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the trackmate dataframe to the cellPLATO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = cp.trackmate_to_cellPLATO(merged_spots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>2. Measurements of morphology and migration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell does migration and morphology measurements for all of the cells at each timepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df, new_factors = cp.measurement_pipeline(comb_df, mixed=cp.MIXED_SCALING, factors_to_timeaverage = cp.ALL_FACTORS) \n",
    "display(new_factors)\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "comb_df, filt_counts = cp.apply_filters(comb_df)\n",
    "\n",
    "# Process a time-averaged DataFrame\n",
    "tavg_df = cp.time_average_trackmate(comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    comb_df.to_csv(cp.SAVED_DATA_PATH + 'comb_df.csv', index=False)\n",
    "    tavg_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tavg_df from csv\n",
    "comb_df = pd.read_csv(cp.SAVED_DATA_PATH + 'comb_df.csv')\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a preview plot of any of these factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackmate_factors = ['RADIUS',\n",
    "                    #  'VISIBILITY',                   \n",
    "                    # 'MANUAL_SPOT_COLOR',\n",
    "                    'MEAN_INTENSITY_CH1',\n",
    "                    'MEDIAN_INTENSITY_CH1',\n",
    "                    'MIN_INTENSITY_CH1',\n",
    "                    'MAX_INTENSITY_CH1',\n",
    "                    'TOTAL_INTENSITY_CH1',\n",
    "                    'STD_INTENSITY_CH1',\n",
    "                    'CONTRAST_CH1',\n",
    "                    'SNR_CH1',\n",
    "                    'ELLIPSE_X0',\n",
    "                    'ELLIPSE_Y0',\n",
    "                    'ELLIPSE_MAJOR',\n",
    "                    'ELLIPSE_MINOR',\n",
    "                    'ELLIPSE_THETA',\n",
    "                    'ELLIPSE_ASPECTRATIO',\n",
    "                    'AREA',\n",
    "                    'PERIMETER',\n",
    "                    'CIRCULARITY',\n",
    "                    'SOLIDITY',\n",
    "                    'SHAPE_INDEX',\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=cp.plots_of_differences_sns(tavg_df,factor='SHAPE_INDEX')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filt_df or comb_df depending on what you want to see\n",
    "f=cp.multi_condition_timeplot(comb_df, factor='CONTRAST_CH1')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: do filtering on the data (on top of what has been stated in the config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined filters in dict {factor:(min, max)}\n",
    "\n",
    "data_filters = {\n",
    "#   \"speed\": (10, 100),\n",
    "  \"area\": (50, 10000),\n",
    "#    \"frame\": (0, 450), # Warning: range will change if self-normalized\n",
    "#   \"ntpts\": (12,1800)\n",
    "}\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "filt_df, filt_counts = cp.apply_filters(comb_df,how='any', filter_dict=data_filters)\n",
    "\n",
    "fig = cp.visualize_filtering(filt_df, filt_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all metrics\n",
    "\n",
    "This cell makes comparative plots for every single metric and saves them in your output folder\n",
    "\n",
    "* Plots of difference\n",
    "* Timeplots of difference\n",
    "* Marginal xy plots\n",
    "* Simple bar plots\n",
    "* Superplots - useful for comparing between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Check that you are happy with your extra filtering before continuing\n",
    "Run the next cell on the filtered dataframe or the unfiltered dataframe once you are ready\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs plots of all metrics for all factors\n",
    "cp.comparative_visualization_pipeline(comb_df, num_factors=trackmate_factors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>3. Definition of single timepoint behavioural clusters using UMAP and HDBSCAN</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: for new datasets perform correlation analysis to understand which factors correlate to one another\n",
    "\n",
    "This may aid in choosing the most important factors, aiding clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = comb_df\n",
    "cp.correlation_matrix_heatmap(df_in, factors = cp.ALL_FACTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: use variance thresholder for further insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dr_factors = cp.variance_threshold(comb_df, threshold_value=0.03)\n",
    "chosen_dr_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: define a new list of dr_factors to use for UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_FACTORS = trackmate_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform UMAP and cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, do UMAP, save the new df and plot the UMAP\n",
    "\n",
    "Well separated clusters depend mostly on 1. the input factors and 2. the umap_nn setting\n",
    "\n",
    "You can change both, depending on the nature of your data, in order to achieve a reasonable level of separation of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### User alterable parameters ######\n",
    "tsne_perp=150\n",
    "umap_nn = 30 #umap nearest neighbours\n",
    "min_dist = 0.0 #umap minimum distance (usually keep this at 0 or very low)\n",
    "n_components = 3 # number of umap dimensions to calculate\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "dr_df = cp.dr_pipeline_multiUMAPandTSNE(comb_df, \n",
    "                    dr_factors=DR_FACTORS,\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'minmax',) # A number of scaling methods are available: 'choice', 'minmax', 'standard', 'robust', 'normalize', 'quantile', 'maxabs', 'yeo-johnson', 'box-cox'\n",
    "\n",
    "dr_df.to_csv(cp.SAVED_DATA_PATH + 'dr_df.csv', index=False) # Saves the df\n",
    "\n",
    "cp.plot_3D_scatter(dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier='dr_df' + '_byCONDITION_',dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, identify clusters and exemplar cells using HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User adjustable parameters #####\n",
    "min_cluster_size = 1000\n",
    "min_samples = 500\n",
    "cluster_by = 'UMAPNDIM' # UMAPNDIM = default, clusters on UMAPs. NDIM = alternate, clusters on all dimensions\n",
    "metric = 'euclidean' # See https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan.HDBSCAN for options\n",
    "#######################################\n",
    "\n",
    "lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_by=cluster_by,  metric=metric)\n",
    "\n",
    "lab_dr_df.name='lab_dr_df'\n",
    "name = lab_dr_df.name\n",
    "\n",
    "lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv', index=False)\n",
    "exemplar_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv', index=False)\n",
    "\n",
    "cp.plot_3D_scatter(lab_dr_df_30, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name + '_byCLUSTERID___',dotsize = 0.01, alpha=0.1, markerscale = 100) #color = label or condition   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp.plot_3D_scatter_dev(exemplar_cell_tracks_df_500_gold, 'UMAP1', 'UMAP2', 'UMAP3', colorby='uniq_id', ticks=False, identifier='exemplar_cell_tracks_df_500_gold' + '_byCONDITION___',dotsize = 20, alpha=0.1, markerscale = 5) #color = label or condition  2\n",
    "# cp.plot_3D_scatter_dev(tptlabel_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='uniq_id', ticks=False, identifier='exemplar_cell_tracks_df_500_gold' + '_byCONDITION___',dotsize = 0.01, alpha=0.1, markerscale = 5) #color = label or condition  2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then plot the 'fingerprint' plot of percentage in each cluster per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new combo\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(lab_dr_df) \n",
    "display(cluster_purity_df)\n",
    "f = cp.purityplot_percentcluspercondition(lab_dr_df, cluster_purity_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: explore the clusters with interactive 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.interactive_plot_3D_UMAP(df=lab_dr_df,colorby = 'Condition_shortlabel', symbolby = 'Condition_shortlabel', what = ' AllTimeUMAPwithclusters') # TavgUMAPwithclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: all other conditions colored grey, chosen condition in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_dr_df\n",
    "\n",
    "condlist = df['Condition_shortlabel'].unique().tolist() #get unique list of conditions from df\n",
    "print(condlist) # show the condition list\n",
    "# chosen_condition = '' #specify a chosen condition from the list\n",
    "chosen_condition = condlist[0] # or choose the first one\n",
    "print(chosen_condition)\n",
    "\n",
    "cp.interactive_plot_3D_UMAP_chosen_condition(df, chosen_condition, opacity_grey=0.01, marker_size_all=2,) #change opacity and marker size to suit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make UMAP plots colored by metric contributors - the more intense the color, the higher the contribution the metric to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one colors per metric\n",
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = False, samplethedf = False)\n",
    "#second one colors per condition\n",
    "# cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=tptlabel_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "#                                                    identifier='inferno', colormap='inferno', coloredbycondition = True, samplethedf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP then HDBSCAN on the tavg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at the moment, just do this step as it is needed for compatibility later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_perp=150\n",
    "umap_nn = 20#4#60\n",
    "min_dist = 0.0 #0.15 \n",
    "n_components = 3\n",
    "\n",
    "tavg_dr_df = cp.dr_pipeline_multiUMAPandTSNE(tavg_df, \n",
    "                    dr_factors=new_DR_FACTORS,# new_DR_FACTORS # DR_FACTORS #only_tmeans # cp.DR_FACTORS\n",
    "                    n_components = n_components,\n",
    "                    umap_nn=umap_nn,\n",
    "                    min_dist= min_dist,\n",
    "                    scalingmethod = 'choice',) # log2minmax # powertransformer #minmax\n",
    "\n",
    "lab_tavg_dr_df, exemplar_tavg_df=cp.hdbscan_clustering(tavg_dr_df, min_cluster_size=50,min_samples=50,cluster_by='UMAPNDIM',  metric='euclidean', plot=False) # \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Save your dataframes so you can come back to this step if necessary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_dr_df.csv', index=False)\n",
    "    lab_tavg_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv', index=False)\n",
    "    exemplar_tavg_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this function to put the labels into the lab_tavg_lab_dr_df. Slow function. Can update search by uniq_id alone...\n",
    "\n",
    "lab_tavg_lab_dr_df=cp.add_tavglabel_todf(lab_dr_df, lab_tavg_dr_df)\n",
    "lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_lab_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the plasticity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = cp.count_cluster_changes_with_tavg(lab_tavg_lab_dr_df)\n",
    "tptlabel_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes(df, identifier='\\_allcells', maxy=4) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_countplots(df, identifier='_allcells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_allcells', miny=None, maxy=None, t_window_multiplier = cp.T_WINDOW_MULTIPLIER, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, choose a number of exemplar cells to pick out from the exemplar cell list to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of exemplars to display for each cluster\n",
    "n=2\n",
    "exemplar_df = exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=220 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=3, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, to visualize single cells with many timepoints, select cells with lots of timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User inputs ####\n",
    "whole_df = tptlabel_dr_df\n",
    "exemplar_df = exemplar_df\n",
    "numberofdesiredtimepoints = int(whole_df['ntpts'].mean())\n",
    "# numberofdesiredtimepoints = 200\n",
    "numberofcellspercluster = 40\n",
    "num_clusters_whole_dataset = len(whole_df['label'].unique())\n",
    "\n",
    "override = int((numberofcellspercluster*num_clusters_whole_dataset)*0.7)\n",
    "#####################\n",
    "\n",
    "exemplar_df_filt, exemplar_cell_tracks_df = cp.filter_exemplars(whole_df=whole_df, exemplar_df = exemplar_df, numberofdesiredtimepoints = numberofdesiredtimepoints, \n",
    "                                                                    numberofcellspercluster = numberofcellspercluster, override = override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=exemplar_cell_tracks_df\n",
    "# cp.plot_cumulative_plasticity_changes_test2(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = True) #deprecated, use the small multiples version\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot any factor as small multiples from the exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exemplar_cell_tracks_df\n",
    "whichcolumntoplot = 'label'\n",
    "\n",
    "cp.plot_small_multiples(df, whichcolumntoplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re'disambiguate the new exemplar df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "df= tptlabel_dr_df #from the all analysis part\n",
    "exemp_df=exemplar_df #from the cluster analysis part.\n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters_topdictionary(df_in=tptlabel_dr_df,  howmanyfactors=10, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint_dev(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONPROPS_LIST = ['area',\n",
    "                    # 'bbox_area',\n",
    "                    'eccentricity',\n",
    "                    'equivalent_diameter',\n",
    "                    # 'extent',\n",
    "                    'filled_area',\n",
    "                    'major_axis_length',\n",
    "                    'minor_axis_length',\n",
    "                    # 'orientation',\n",
    "                    'perimeter',\n",
    "                    #  'solidity'\n",
    "                     ]\n",
    "\n",
    "MIG_FACTORS = ['euclidean_dist',     \n",
    "                'cumulative_length', \n",
    "                'speed',\n",
    "                # 'orientedness', \n",
    "                # 'directedness',\n",
    "                # 'turn_angle',\n",
    "                'endpoint_dir_ratio',\n",
    "                # 'dir_autocorr',\n",
    "                'outreach_ratio',\n",
    "                'MSD',                \n",
    "                'max_dist',           \n",
    "                # 'glob_turn_deg',\n",
    "                'arrest_coefficient']\n",
    "\n",
    "ADDITIONAL_FACTORS = ['aspect', 'rip_L'] # 'rip_p', 'rip_K', \n",
    "\n",
    "\n",
    "newnew_DR_FACTORS = REGIONPROPS_LIST + MIG_FACTORS + ADDITIONAL_FACTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>4. Trajectory measurement: Damerau-Levenshtein</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter the tptlabel_dr_df to include only a subset of data of similar timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 200\n",
    "high = 220\n",
    "\n",
    "tptlabel_dr_df_filt = tptlabel_dr_df[tptlabel_dr_df['ntpts'].between(low, high)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the filtered data reflects the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorchoice = 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes timeplots of the unfiltered and filtered data\n",
    "\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df, factorchoice)\n",
    "f.show()\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df_filt, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of difference of the unfiltered and filtered data\n",
    "f = cp.plots_of_differences_sns(tavg_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_trajectory_df = cp.time_average(tptlabel_dr_df)\n",
    "f = cp.plots_of_differences_sns(tavg_trajectory_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Damerau-Levenshtein analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt\n",
    "distance_matrix_dameraulev = cp.calculate_edit_distances(df,distancemetric = 'dameraulev', print_interval=10000) #fastdtw # dameraulev # mongeelkan\n",
    "print(distance_matrix_dameraulev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distance matrix\n",
    "# np.save(cp.SAVED_DATA_PATH + 'distance_matrix_dameraulev.npy', distance_matrix_dameraulev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a UMAP/HDBSCAN parameter sweep, and select plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sweep'''\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "for n_neighbors in [8, 10, 12]:\n",
    "    for min_samples in [5,8,10, 15, 30, 40]:\n",
    "        for min_cluster_size in [5,8,10, 15, 30, 40]:\n",
    "            print(f'min_samples = {min_samples}')\n",
    "            print(f'min_cluster_size = {min_cluster_size}')\n",
    "            print(f'n_neighbors = {n_neighbors}')\n",
    "            tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    "             do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Chosen UMAP and HDBSCAN parameters'''\n",
    "\n",
    "min_samples = 30\n",
    "min_cluster_size = 20\n",
    "n_neighbors = 12\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "\n",
    "print(f'min_samples = {min_samples}')\n",
    "print(f'min_cluster_size = {min_cluster_size}')\n",
    "print(f'n_neighbors = {n_neighbors}')\n",
    "tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    " do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fingerprint plot of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(df, cluster_label='trajectory_id') \n",
    "f = cp.purityplot_percentcluspercondition(df, cluster_purity_df, cluster_label='trajectory_id', dotsize = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disambiguate the trajectory clustered cells:\n",
    " 1) Make an exemplar_df_trajectories containing example rows\n",
    " 2) Get the full tracks from those rows and make exemplar_df_trajectories_fulltrack\n",
    " 2) Disambiguate with exemplar_df_trajectories\n",
    " 3) Plot multiples with exemplar_df_trajectories_fulltrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "exemplar_df_trajectories, exemplar_df_trajectories_fulltrack  = cp.make_exemplar_df_basedon_trajectories(df, cells_per_traj=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_tracks_df = pd.read_csv(cp.SAVED_DATA_PATH + 'full_tracks_df.csv')\n",
    "df = exemplar_df_trajectories_fulltrack\n",
    "cp.plot_trajectories(df=exemplar_df_trajectories_fulltrack, global_y=True, global_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "\n",
    "df= tptlabel_dr_df_filt_clusteredtrajectories \n",
    "exemp_df=exemplar_df_trajectories \n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=2, dr_factors= newnew_DR_FACTORS) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent fingerprint plot for cluster IDs per TRAJECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-12-2023.csv')\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.fingerprintplot_clusters_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasticity of cells per trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-17-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.plasticity_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes_trajectories(df, identifier='\\_allcells', maxy=9 , t_window_multiplier = 1) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animations of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_trajectory_animations(df, exemplar_df_trajectories, number_of_trajectories=2, colormode='cluster') # singlecluster, cluster, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a number of example cells from each trajectory ID to map back on to the data and display as stacks of PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trajectories = 10 # Select a number of trajectories to plot\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "trajectory_ids = df['trajectory_id'].unique()\n",
    "\n",
    "uniq_id_choices_list = []\n",
    "\n",
    "for trajectory_id_choice in trajectory_ids:\n",
    "    # for each trajectory_id, get a list of possible uniq_ids from the df\n",
    "    uniq_id_choices = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['trajectory_id']==trajectory_id_choice]['uniq_id'].values\n",
    "    # Make sure each once is unique in that list\n",
    "    uniq_id_choices = np.unique(uniq_id_choices)\n",
    "    # choose a number of random uniq_ids from that list based on number_of_trajectories\n",
    "    uniq_id_choices = np.random.choice(uniq_id_choices, number_of_trajectories)\n",
    "    # append each choice to a list\n",
    "    uniq_id_choices_list.append(uniq_id_choices)\n",
    "# flatten the list\n",
    "chosen_uniq_ids = [item for sublist in uniq_id_choices_list for item in sublist]\n",
    "    \n",
    "print(chosen_uniq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_png_behaviour_trajectories(df,chosen_uniq_ids,XYRange = 300, follow_cell = False, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_raw_cell_pngstacks(df,chosen_uniq_ids,XYRange = 220, follow_cell=False, invert=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellPLATO_gitversion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
