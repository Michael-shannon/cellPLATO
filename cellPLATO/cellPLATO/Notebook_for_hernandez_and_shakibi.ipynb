{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellPLATO | Cell Plasticity Analysis Tool (Trackmate version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fill in the config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, make sure your data is set up in the following two tiered format:\n",
    "\n",
    "        Master\n",
    "            ├── Condition 1\n",
    "            │   ├── Replicate 1\n",
    "            |   |       ├── tracks.h5\n",
    "            │   ├── Replicate 2\n",
    "            |   |       ├── tracks.h5            \n",
    "            │   └── Replicate 3\n",
    "            |           └── tracks.h5            \n",
    "            │  \n",
    "            └── Condition 2,\n",
    "                ├── Replicate 1\n",
    "                |       ├── tracks.h5\n",
    "                ├── Replicate 2\n",
    "                |       ├── tracks.h5            \n",
    "                └── Replicate 3\n",
    "                        └── tracks.h5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Set your kernel to 'cellPLATO' before continuing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>1. Start by importing packages for cellPLATO</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes cellPLATO itself, and all of the modules you will need\n",
    "\n",
    "* Import these packages, checking that you have them\n",
    "* We're also importing a lot of the modules in cellPLATO, if this cell runs successfully, you are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellPLATO as cp\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.cm as cm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import requests\n",
    "\n",
    "\n",
    "OVERWRITE_DATAFRAMES = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import your experiment list\n",
    "\n",
    "Check that the list generated in the next cell contains your conditions and replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment list from the experiments listed in the config \n",
    "exp_list = cp.populate_experiment_list()\n",
    "display(exp_list)\n",
    "print(cp.SAVED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the trackmate dataframe\n",
    "\n",
    "This part was inspired by Guillaume Jacquemet's trackmate processing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spots_df, spots_metadata = cp.load_and_populate(r'.*spots.*\\.csv')\n",
    "\n",
    "merged_tracks_df, tracks_metadata = cp.load_and_populate(r'.*tracks.*\\.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the trackmate dataframe to the cellPLATO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = cp.trackmate_to_cellPLATO(merged_spots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time window decision: before proceeding, let's figure out what time window to use for the cellPLATO migration features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your current time window settings\n",
    "analysis = cp.analyze_time_window_settings(comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to change the time window settings, you can do so in the config.py file at this line:\n",
    "# MigrationTimeWindow_minutes = 12.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>2. Measurements of morphology and migration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell does migration and morphology measurements for all of the cells at each timepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df, new_factors = cp.measurement_pipeline(comb_df, mixed=cp.MIXED_SCALING, factors_to_timeaverage = cp.ALL_FACTORS) \n",
    "display(new_factors)\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "comb_df, filt_counts = cp.apply_filters(comb_df)\n",
    "\n",
    "# Process a time-averaged DataFrame\n",
    "tavg_df = cp.time_average_trackmate(comb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the trackmate data had missing frames due to missed detections, one can fill them with \n",
    "### interpolation or split the tracks.\n",
    "Here I chose interpolation. I believe this is an option in trackmate, and is certainly an option in other tracking methodologies. If it is not done upstream, you can do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix gaps with chosen method\n",
    "comb_df = cp.fix_track_gaps(\n",
    "    comb_df, \n",
    "    method='fill',  # 'fill', 'split', or 'auto'\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    comb_df.to_csv(cp.SAVED_DATA_PATH + 'comb_df.csv', index=False)\n",
    "    tavg_df.to_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tavg_df from csv\n",
    "comb_df = pd.read_csv(cp.SAVED_DATA_PATH + 'comb_df.csv')\n",
    "tavg_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tavg_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trackmate factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrackMate morphological and intensity features\n",
    "TRACKMATE_FEATURES = [\n",
    "    'RADIUS', 'VISIBILITY', 'MEAN_INTENSITY_CH1',\n",
    "    'MEDIAN_INTENSITY_CH1', 'MIN_INTENSITY_CH1', 'MAX_INTENSITY_CH1',\n",
    "    'TOTAL_INTENSITY_CH1', 'STD_INTENSITY_CH1', 'MEAN_INTENSITY_CH2',\n",
    "    'MEDIAN_INTENSITY_CH2', 'MIN_INTENSITY_CH2', 'MAX_INTENSITY_CH2',\n",
    "    'TOTAL_INTENSITY_CH2', 'STD_INTENSITY_CH2', 'CONTRAST_CH1', 'SNR_CH1',\n",
    "    'CONTRAST_CH2', 'SNR_CH2', 'ELLIPSE_X0', 'ELLIPSE_Y0', 'ELLIPSE_MAJOR',\n",
    "    'ELLIPSE_MINOR', 'ELLIPSE_THETA', 'ELLIPSE_ASPECTRATIO', 'AREA',\n",
    "    'PERIMETER', 'CIRCULARITY', 'SOLIDITY', 'SHAPE_INDEX'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define cellPLATO migration factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migration parameters calculated by cellPLATO\n",
    "MIGRATION_FEATURES = [\n",
    "    'euclidean_dist', 'segment_length', 'cumulative_length', 'speed',\n",
    "    'orientedness', 'directedness', 'turn_angle', 'endpoint_dir_ratio',\n",
    "    'dir_autocorr', 'outreach_ratio', 'MSD', 'max_dist', 'glob_turn_deg',\n",
    "    'arrest_coefficient', 'rip_p', 'rip_K', 'rip_L'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put them all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_FACTORS = TRACKMATE_FEATURES + MIGRATION_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot of any factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=cp.plots_of_differences_sns(tavg_df,factor='SHAPE_INDEX')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a timeplot of any factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filt_df or comb_df depending on what you want to see\n",
    "f=cp.multi_condition_timeplot(comb_df, factor='CONTRAST_CH1')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: do filtering on the data (on top of what has been stated in the config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined filters in dict {factor:(min, max)}\n",
    "\n",
    "data_filters = {\n",
    "#   \"speed\": (10, 100),\n",
    "  \"AREA\": (1, 10000),\n",
    "#    \"frame\": (0, 450), # Warning: range will change if self-normalized\n",
    "  # \"ntpts\": (2,1800)\n",
    "}\n",
    "\n",
    "# Returns a filtered dataframe, while also adding included column to comb_df\n",
    "filt_df, filt_counts = cp.apply_filters(comb_df,how='any', filter_dict=data_filters)\n",
    "\n",
    "fig = cp.visualize_filtering(filt_df, filt_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all metrics\n",
    "\n",
    "This cell makes comparative plots for every single metric and saves them in your output folder\n",
    "\n",
    "* Plots of difference\n",
    "* Timeplots of difference\n",
    "* Marginal xy plots\n",
    "* Simple bar plots\n",
    "* Superplots - useful for comparing between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Check that you are happy with your extra filtering before continuing\n",
    "Run the next cell on the filtered dataframe or the unfiltered dataframe once you are ready\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs plots of all metrics for all factors\n",
    "# cp.comparative_visualization_pipeline(comb_df, num_factors=DR_FACTORS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>3. Definition of single timepoint behavioural clusters using UMAP and HDBSCAN</h1>\n",
    "</div>\n",
    "\n",
    "Here, you should pay attention to which factors you choose for dimensionality reduction. The following section provides some ways to aid that decision, to be combined with biological knowledge as to which factors are important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform correlation analysis to understand which factors correlate to one another\n",
    "\n",
    "This helps to avoid picking factors that are very similar to one another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = comb_df\n",
    "cp.correlation_matrix_heatmap(df_in, factors = cp.ALL_FACTORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: use variance thresholder for further insight\n",
    "\n",
    "This works using scikitlearns variance measurement\n",
    "\n",
    "Importantly, you have to centre scale the data prior to doing this, in order to match the dimension reduction and cluster analysis you do later.\n",
    "\n",
    "Several scaling methods are avaialable. In order to use log2 and minmax scaling (which is called 'choice') and was used in the paper, you can automatically figure out which features are chosen to be log2 transformed and then minmax-ed, versus simply minmax-ed.\n",
    "\n",
    "Printouts detail why a decision was made, as well as histograms to show the scale, skew and spread of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First analyze your factors to optimize scaling\n",
    "factor_analysis = cp.analyze_factors_for_choice_scaling(\n",
    "    df=comb_df, \n",
    "    factors_list=DR_FACTORS,\n",
    "    show_distributions=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can apply the correct scaling to the metrics, and measure their variance contribution. Scikitlearn uses:\n",
    "\n",
    "Variance = Σ(xᵢ - μ)² / (n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then apply variance threshold with optimized scaling\n",
    "chosen_dr_factors = cp.variance_threshold(\n",
    "    df_in=comb_df,\n",
    "    threshold_value=0.03, \n",
    "    dr_factors=DR_FACTORS,\n",
    "    scaling_method='choice',\n",
    "    factors_to_transform=factor_analysis['suggested_to_transform'],\n",
    "    factors_not_to_transform=factor_analysis['suggested_not_to_transform']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will remove rip_p and rip_k, only using ripleys L as it is the most useful!\n",
    "chosen_dr_factors.remove('rip_p')\n",
    "chosen_dr_factors.remove('rip_K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: there are other scaling options available! This combo of log2 and minmax was what I used in the paper. Others include standardscaler, the most popular. Search 'center scaling' to find out more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaNs. \n",
    "\n",
    "There are NaNs in the data. Some are normal, at the start and end of tracks due to time windows not being large enough. But there were many more on top of this.\n",
    "I found from the following analysis that there are sometimes missing consecutive frames in tracks, due to the way trackmate works. Run the following function to show that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual tracks to see where NaNs come from\n",
    "track_analysis = cp.analyze_individual_tracks_for_nans(comb_df, track_id_col='uniq_id', n_tracks_to_analyze=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the NaN problem\n",
    "investigation_results = cp.investigate_nan_causes(comb_df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a single track in detail\n",
    "single_track = comb_df[comb_df['uniq_id'] == comb_df['uniq_id'].iloc[0]].sort_values('frame')\n",
    "print(\"Track frames:\", single_track['frame'].values[:20])\n",
    "print(\"Track length:\", len(single_track))\n",
    "print(\"Frame range:\", single_track['frame'].min(), \"to\", single_track['frame'].max())\n",
    "print(\"Has gaps:\", len(set(range(int(single_track['frame'].min()), int(single_track['frame'].max()) + 1))) != len(single_track))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve gaps in your data, one can interpolate positions where there are gaps in tracks 'fill' or one can 'split' tracks to make them into new IDs. If you are going to do this, you need to go back to the beginning and do the calculations on the new dataframe.\n",
    "\n",
    "### Fix gaps with chosen method\n",
    "comb_df_fixed = cp.fix_track_gaps(\n",
    "    comb_df, \n",
    "    method='fill',  # 'fill', 'split', or 'auto'\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once there are no gaps in the data due to missed detections, there will still be a few gaps at the start and end of tracks due to the use of time windows.\n",
    "This is a trade off for using time windows, which gives the data much less bias, but loses a bit of data (3 frames at the start and end, in this case).\n",
    "You can either drop those rows entirely or remove the cellPLATO migration factors, calculated over time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just analyze without changing anything\n",
    "df_clean, factors_clean, report = cp.handle_nan_for_dr(\n",
    "    comb_df, \n",
    "    DR_FACTORS, \n",
    "    method='drop_rows'\n",
    ")\n",
    "\n",
    "# auto': Drop factors with >nan_threshold% NaN, then drop remaining NaN rows\n",
    "# 'drop_rows': Drop all rows containing any NaN values in DR factors\n",
    "# 'drop_factors': Drop all factors containing any NaN values\n",
    "# 'analyze_only': Just analyze and report, don't modify data nan_threshold : float For 'auto' method: percentage threshold for dropping factors (default 30%) verbose : bool Print detailed information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the suggestions or customize them\n",
    "factors_to_log_transform = factor_analysis['suggested_to_transform']\n",
    "factors_minmax_only = factor_analysis['suggested_not_to_transform']\n",
    "\n",
    "# Update your custom factor lists to only include clean factors\n",
    "factors_to_log_transform_clean = [f for f in factors_to_log_transform if f in factors_clean]\n",
    "factors_minmax_only_clean = [f for f in factors_minmax_only if f in factors_clean]\n",
    "\n",
    "print(f\"Clean factors to log-transform: {len(factors_to_log_transform_clean)}\")\n",
    "print(f\"Clean factors for minmax only: {len(factors_minmax_only_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rip_p and rip_k\n",
    "factors_to_log_transform_clean = [f for f in factors_to_log_transform_clean if f not in ['rip_p', 'rip_k']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices \n",
    "df_clean = df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df_clean.to_csv('df_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe\n",
    "df_clean = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform UMAP and cluster analysis\n",
    "\n",
    "Well separated clusters depend mostly on 1. the input factors and 2. the umap_nn setting\n",
    "\n",
    "You can change both, depending on the nature of your data, in order to achieve a reasonable level of separation of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### User alterable parameters ######\n",
    "tsne_perp=150\n",
    "umap_nn = 30 #umap nearest neighbours\n",
    "min_dist = 0.0 #umap minimum distance (usually keep this at 0 or very low)\n",
    "n_components = 3 # number of umap dimensions to calculate\n",
    "#######################################\n",
    "\n",
    "dr_df = cp.dr_pipeline_multiUMAPandTSNE(\n",
    "    df_clean, \n",
    "    dr_factors=DR_FACTORS,\n",
    "    n_components=n_components,\n",
    "    umap_nn=umap_nn,\n",
    "    min_dist=min_dist,\n",
    "    scalingmethod='choice',  # Use choice scaling\n",
    "    factors_to_transform=factors_to_log_transform_clean,      # Your custom list\n",
    "    factors_not_to_transform=factors_minmax_only_clean,       # Your custom list\n",
    "    do_tsne=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.plot_3D_scatter(dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='condition', ticks=False, identifier='dr_df' + '_byCONDITION_',dotsize = 8, alpha=0.2, markerscale = 10) #color = label or condition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dr_df[dr_df.isnull().any(axis=1)])\n",
    "dr_df=dr_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, identify clusters and exemplar cells using HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### User adjustable parameters #####\n",
    "min_cluster_size = 300\n",
    "min_samples = 200\n",
    "cluster_by = 'UMAPNDIM' # UMAPNDIM = default, clusters on UMAPs. NDIM = alternate, clusters on all dimensions\n",
    "metric = 'euclidean' # See https://hdbscan.readthedocs.io/en/latest/api.html#hdbscan.HDBSCAN for options\n",
    "#######################################\n",
    "\n",
    "lab_dr_df, exemplar_df=cp.hdbscan_clustering(dr_df, min_cluster_size=min_cluster_size, min_samples=min_samples, cluster_by=cluster_by,  metric=metric)\n",
    "\n",
    "lab_dr_df.name='lab_dr_df'\n",
    "name = lab_dr_df.name\n",
    "\n",
    "lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv', index=False)\n",
    "exemplar_df.to_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv', index=False)\n",
    "\n",
    "cp.plot_3D_scatter(lab_dr_df, 'UMAP1', 'UMAP2', 'UMAP3', colorby='label', ticks=False, identifier=name + '_byCLUSTERID___',dotsize = 8, alpha=0.2, markerscale = 10) #color = label or condition   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lab_dr_df to a csv file\n",
    "lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint - load the lab_dr_df from the csv file\n",
    "lab_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_dr_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then plot the 'fingerprint' plot of percentage in each cluster per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new combo\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(lab_dr_df) \n",
    "display(cluster_purity_df)\n",
    "f = cp.purityplot_percentcluspercondition(lab_dr_df, cluster_purity_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: explore the clusters with interactive 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.interactive_plot_3D_UMAP(df=lab_dr_df,colorby = 'label', symbolby = 'Condition_shortlabel', what = ' AllTimeUMAPwithclusters') # TavgUMAPwithclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: all other conditions colored grey, chosen condition in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=lab_dr_df\n",
    "\n",
    "condlist = df['Condition_shortlabel'].unique().tolist() #get unique list of conditions from df\n",
    "print(condlist) # show the condition list\n",
    "# chosen_condition = '' #specify a chosen condition from the list\n",
    "chosen_condition = condlist[0] # or choose the first one\n",
    "print(chosen_condition)\n",
    "\n",
    "cp.interactive_plot_3D_UMAP_chosen_condition(df, chosen_condition, opacity_grey=0.3, marker_size_all=5,) #change opacity and marker size to suit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make UMAP plots colored by metric contributors - the more intense the color, the higher the contribution the metric to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one colors per metric\n",
    "cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=lab_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = DR_FACTORS, scalingmethod='choice',\n",
    "                                                   identifier='inferno', colormap='inferno', coloredbycondition = False, samplethedf = False)\n",
    "#second one colors per condition\n",
    "# cp.plot_UMAP_subplots_coloredbymetricsorconditions(df_in=tptlabel_dr_df, x= 'UMAP1', y= 'UMAP2', z = 'UMAP3', n_cols = 5, ticks=False, metrics = cp.ALL_FACTORS, scalingmethod='choice',\n",
    "#                                                    identifier='inferno', colormap='inferno', coloredbycondition = True, samplethedf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform UMAP then HDBSCAN on the tavg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at the moment, just do this step as it is needed for compatibility later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_dr_df['tavg_label'] = np.random.randint(0, 5, lab_dr_df.shape[0])\n",
    "lab_tavg_lab_dr_df = lab_dr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DATAFRAMES = True\n",
    "\n",
    "if OVERWRITE_DATAFRAMES:\n",
    "    lab_tavg_lab_dr_df.to_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load that df\n",
    "lab_tavg_lab_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'lab_tavg_dr_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the plasticity \n",
    "\n",
    "This part counts how many times cells switch between those clusters we defined over their lifetime\n",
    "\n",
    "The function now returns these cluster change metrics:\n",
    "\n",
    "cum_n_changes, cum_n_labels (cumulative)\n",
    "twind_n_changes, twind_n_labels (time-windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = cp.count_cluster_changes_with_tavg(lab_tavg_lab_dr_df)\n",
    "tptlabel_dr_df.to_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes(df, identifier='\\_allcells', maxy=4) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_plasticity_countplots(df, identifier='_allcells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_allcells', miny=None, maxy=None, t_window_multiplier = cp.T_WINDOW_MULTIPLIER, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, choose a number of exemplar cells to pick out from the exemplar cell list to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of exemplars to display for each cluster\n",
    "n=2\n",
    "exemplar_df = exemplar_df.groupby('label').apply(lambda x: x.sample(min(n,len(x)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### No need to run these commented lines if you have already chosen the factors to use using this method above ########################\n",
    "\n",
    "# # Get factor analysis results\n",
    "# factor_analysis = cp.analyze_factors_for_choice_scaling(\n",
    "#     df=comb_df, \n",
    "#     factors_list=DR_FACTORS,\n",
    "#     show_distributions=True\n",
    "# )\n",
    "\n",
    "# # Use the suggestions\n",
    "# factors_to_log_transform = factor_analysis['suggested_to_transform']\n",
    "# factors_minmax_only = factor_analysis['suggested_not_to_transform']\n",
    "\n",
    "# # Clean the factors\n",
    "# factors_to_log_transform_clean = [f for f in factors_to_log_transform if f in factors_clean]\n",
    "# factors_minmax_only_clean = [f for f in factors_minmax_only if f in factors_clean]\n",
    "\n",
    "# Use in contribution_to_clusters\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df = cp.contribution_to_clusters(\n",
    "    df_in=tptlabel_dr_df,  \n",
    "    howmanyfactors=3, \n",
    "    dr_factors=chosen_dr_factors,\n",
    "    scalingmethod='choice',\n",
    "    factors_to_log_transform_clean=factors_to_log_transform_clean,\n",
    "    factors_minmax_only_clean=factors_minmax_only_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't do this without the contours (segmentations), which you don't have here\n",
    "\n",
    "cp.disambiguate_timepoint(df, exemplar_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')\n",
    "exemplar_df = pd.read_csv(cp.SAVED_DATA_PATH + 'exemplar_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, to visualize single cells with many timepoints, select cells with lots of timepoints\n",
    "\n",
    "Filter cells by trajectory length: Only keeps cells that have more than numberofdesiredtimepoints timepoints\n",
    "Sample cells per cluster: Tries to get numberofcellspercluster cells from each cluster\n",
    "Create exemplar datasets: Outputs both a filtered exemplar dataframe and full trajectory tracks for those cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User inputs ####\n",
    "whole_df = tptlabel_dr_df\n",
    "exemplar_df = exemplar_df\n",
    "numberofdesiredtimepoints = int(whole_df['ntpts'].mean())\n",
    "# numberofdesiredtimepoints = 200\n",
    "numberofcellspercluster = 40\n",
    "num_clusters_whole_dataset = len(whole_df['label'].unique())\n",
    "\n",
    "override = int((numberofcellspercluster*num_clusters_whole_dataset)*0.7)\n",
    "#####################\n",
    "\n",
    "# exemplar_df_filt, exemplar_cell_tracks_df = cp.filter_exemplars(whole_df=whole_df, exemplar_df = exemplar_df, numberofdesiredtimepoints = numberofdesiredtimepoints, \n",
    "#                                                                     numberofcellspercluster = numberofcellspercluster, override = override)\n",
    "\n",
    "exemplar_df_filt, exemplar_cell_tracks_df = cp.filter_exemplars(\n",
    "    whole_df=whole_df, \n",
    "    exemplar_df=exemplar_df, \n",
    "    numberofdesiredtimepoints=numberofdesiredtimepoints,\n",
    "    numberofcellspercluster=numberofcellspercluster, \n",
    "    override=override,\n",
    "    verbose=False  # This eliminates most printouts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=exemplar_cell_tracks_df\n",
    "# cp.plot_cumulative_plasticity_changes_test2(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = True) #deprecated, use the small multiples version\n",
    "cp.plot_cumulative_plasticity_changes_main(df, identifier='\\_exemplars_only_3_df__', miny=None, maxy=None, t_window_multiplier = 1, plotallcells = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot any factor as small multiples from the exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exemplar_cell_tracks_df\n",
    "whichcolumntoplot = 'label'\n",
    "\n",
    "cp.plot_small_multiples(df, whichcolumntoplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2>4. Trajectory measurement: Damerau-Levenshtein</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of points per unique id (uniq_id)\n",
    "temp_df = dr_df.copy()\n",
    "numpoints_df = temp_df.groupby('uniq_id').size().reset_index(name='numpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of numpoints\n",
    "plt.hist(numpoints_df['numpoints'], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter the tptlabel_dr_df to include only a subset of data of similar timescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 5\n",
    "high = 60\n",
    "\n",
    "tptlabel_dr_df_filt = tptlabel_dr_df[tptlabel_dr_df['ntpts'].between(low, high)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the filtered data reflects the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorchoice = 'speed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes timeplots of the unfiltered and filtered data\n",
    "\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df, factorchoice)\n",
    "f.show()\n",
    "f=cp.multi_condition_timeplot(tptlabel_dr_df_filt, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of difference of the unfiltered and filtered data\n",
    "f = cp.plots_of_differences_sns(tavg_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavg_trajectory_df = cp.time_average(tptlabel_dr_df)\n",
    "f = cp.plots_of_differences_sns(tavg_trajectory_df, factorchoice)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Damerau-Levenshtein analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt\n",
    "distance_matrix_dameraulev = cp.calculate_edit_distances(df,distancemetric = 'dameraulev', print_interval=10000) #fastdtw # dameraulev # mongeelkan\n",
    "print(distance_matrix_dameraulev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distance matrix\n",
    "# np.save(cp.SAVED_DATA_PATH + 'distance_matrix_dameraulev.npy', distance_matrix_dameraulev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a UMAP/HDBSCAN parameter sweep, and select plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sweep'''\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "for n_neighbors in [8, 10, 12]:\n",
    "    for min_samples in [5,8,10, 15, 30, 40]:\n",
    "        for min_cluster_size in [5,8,10, 15, 30, 40]:\n",
    "            print(f'min_samples = {min_samples}')\n",
    "            print(f'min_cluster_size = {min_cluster_size}')\n",
    "            print(f'n_neighbors = {n_neighbors}')\n",
    "            tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    "             do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Chosen UMAP and HDBSCAN parameters'''\n",
    "\n",
    "min_samples = 8\n",
    "min_cluster_size = 6\n",
    "n_neighbors = 5\n",
    "\n",
    "df = tptlabel_dr_df_filt\n",
    "\n",
    "print(f'min_samples = {min_samples}')\n",
    "print(f'min_cluster_size = {min_cluster_size}')\n",
    "print(f'n_neighbors = {n_neighbors}')\n",
    "tptlabel_dr_df_filt_clusteredtrajectories = cp.cluster_sequences(df, distance_matrix_dameraulev,\n",
    " do_umap=True, eps=0.1, min_samples=min_samples, min_cluster_size=min_cluster_size, n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fingerprint plot of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "cluster_purity_df = cp.purity_pointsinclusterspercondition(df, cluster_label='trajectory_id') \n",
    "f = cp.purityplot_percentcluspercondition(df, cluster_purity_df, cluster_label='trajectory_id', dotsize = 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disambiguate the trajectory clustered cells:\n",
    " 1) Make an exemplar_df_trajectories containing example rows\n",
    " 2) Get the full tracks from those rows and make exemplar_df_trajectories_fulltrack\n",
    " 2) Disambiguate with exemplar_df_trajectories\n",
    " 3) Plot multiples with exemplar_df_trajectories_fulltrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "exemplar_df_trajectories, exemplar_df_trajectories_fulltrack  = cp.make_exemplar_df_basedon_trajectories(df, cells_per_traj=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_tracks_df = pd.read_csv(cp.SAVED_DATA_PATH + 'full_tracks_df.csv')\n",
    "FONT_SIZE = 10\n",
    "df = exemplar_df_trajectories_fulltrack\n",
    "cp.plot_trajectories(df=exemplar_df_trajectories_fulltrack, global_y=True, global_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=300 #\n",
    "\n",
    "\n",
    "df= tptlabel_dr_df_filt_clusteredtrajectories \n",
    "exemp_df=exemplar_df_trajectories \n",
    "\n",
    "top_dictionary, contributions_df_singletpoints, scaled_df=cp.contribution_to_clusters(df_in=tptlabel_dr_df,  howmanyfactors=2, dr_factors= chosen_dr_factors) #BEFORE disambiguate_tavg(), then: lab_tavg_dr_df BEFORE disambiguate_timepoint(), then: #tptlabel_dr_df \n",
    "cp.plot_cluster_averages(top_dictionary, df, scaled_df)\n",
    "result_df = cp.create_cluster_averages_table(top_dictionary, df, scaled_df)\n",
    "# same as before - this can't be run (below, commented out) without the contours (segmentations), which you don't have here\n",
    "# cp.disambiguate_timepoint(df, exemp_df, scaled_df, top_dictionary=top_dictionary, XYRange=size,boxoff=True, trajectory = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percent fingerprint plot for cluster IDs per TRAJECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-12-2023.csv')\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.fingerprintplot_clusters_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plasticity of cells per trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tptlabel_dr_df_filt_clusteredtrajectories = pd.read_csv(cp.SAVED_DATA_PATH + 'tptlabel_dr_df_filt_clusteredtrajectories_FINAL_10-17-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.plasticity_per_trajectory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tptlabel_dr_df_filt_clusteredtrajectories\n",
    "# all='\\_allcells'\n",
    "cp.plot_plasticity_changes_trajectories(df, identifier='\\_allcells', maxy=9 , t_window_multiplier = 1) #problem with NaNs in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animations of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_trajectory_animations(df, exemplar_df_trajectories, number_of_trajectories=2, colormode='cluster') # singlecluster, cluster, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a number of example cells from each trajectory ID to map back on to the data and display as stacks of PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trajectories = 10 # Select a number of trajectories to plot\n",
    "\n",
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "\n",
    "trajectory_ids = df['trajectory_id'].unique()\n",
    "\n",
    "uniq_id_choices_list = []\n",
    "\n",
    "for trajectory_id_choice in trajectory_ids:\n",
    "    # for each trajectory_id, get a list of possible uniq_ids from the df\n",
    "    uniq_id_choices = tptlabel_dr_df_filt_clusteredtrajectories[tptlabel_dr_df_filt_clusteredtrajectories['trajectory_id']==trajectory_id_choice]['uniq_id'].values\n",
    "    # Make sure each once is unique in that list\n",
    "    uniq_id_choices = np.unique(uniq_id_choices)\n",
    "    # choose a number of random uniq_ids from that list based on number_of_trajectories\n",
    "    uniq_id_choices = np.random.choice(uniq_id_choices, number_of_trajectories)\n",
    "    # append each choice to a list\n",
    "    uniq_id_choices_list.append(uniq_id_choices)\n",
    "# flatten the list\n",
    "chosen_uniq_ids = [item for sublist in uniq_id_choices_list for item in sublist]\n",
    "    \n",
    "print(chosen_uniq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_png_behaviour_trajectories(df,chosen_uniq_ids,XYRange = 300, follow_cell = False, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tptlabel_dr_df_filt_clusteredtrajectories\n",
    "cp.make_raw_cell_pngstacks(df,chosen_uniq_ids,XYRange = 220, follow_cell=False, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellPLATO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
